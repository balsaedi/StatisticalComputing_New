[["index.html", "Statistical Computing Chapter 1 Reviews 1.1 Data Types and Structures 1.2 Data Importing and Exporting 1.3 Data Cleaning 1.4 Data Visualization 1.5 Hands-on Exercises", " Statistical Computing Dr. Basim Alsaedi 2025-02-06 Chapter 1 Reviews 1.1 Data Types and Structures 1.1.1 Data Types There are different kinds of values in R that can be manipulated in variables. They most commonly used are; strings, numerics(integers and floats) and boolean values. The function class can be used to find the data type. Try it! Before diving deep into data types, lets create a value with a random value for instance age and finds its data type. age &lt;- 27 class(age) ## [1] &quot;numeric&quot; The age is a \"numeric\" data type variable, interesting? Lets explore different data types and their examples; Integers: These are whole numbers without dev=cimal point(e.g., 10, -5). In R, it is specified with the L suffix like 10L. Floats: These are numbers with decimal points(e.g. 3.14, -2.718). R refers them as numerics. Boolean(Logical): True or False values, represented as TRUE or FALSE in R. They are crucial in conditional statements. Strings(Character): These are text values enclosed in quotes(e.g. \"Hello world\" , names like \"John\", \"Mustafa\", \"Patel\", variable names like \"age\", \"gender\", \"salary\") You will often deal with mixed data types when analyzing real-world data sets therefore understanding these will help you handles any data set! Examples Lets have some fun! We will create different variables and find their data types; age &lt;- 34L age &lt;- 34L class(age) ## [1] &quot;integer&quot; weight &lt;- 68.2 weight &lt;- 68.2 class(weight) ## [1] &quot;numeric&quot; name &lt;- \"Mustafa\" name &lt;- &quot;Mustafa&quot; class(name) ## [1] &quot;character&quot; is_winter &lt;- FALSE is_winter &lt;- FALSE class(is_winter) ## [1] &quot;logical&quot; You see how simple it is to find the data type of different variables in R! Remember the class function returns any number whether with decimal or whole as \"numeric\". It only returns \"integer\" when there is a suffix L. Practical Exercise Try out the practical exercise below to test your understanding in data types Find the data type of 98.03 using class() function. Assign the value 98.03 to variable height and find data type of height. There are 27 goats in a field, assign the quantity of goats to a variable goats and find the data type of the variable goats. Remember to add suffix L to the value 27. Find the data type of the value \"school\" using the class() function. Assign your first name to a variable firstname and find its data type. Remember to enclose it in quotation marks Create a variable is_student and assign it the value TRUE. Use the class() function to find its data type. Solution Find the data type of 98.03 using class() function. class(98.03) ## [1] &quot;numeric&quot; Assign the value 98.03 to variable height and find data type of height. height &lt;- 98.03 class(height) ## [1] &quot;numeric&quot; There are 27 goats in a field, assign the quantity of goats to a variable goats and find the data type of the variable goats. Remember to add suffix L to the value 27. goats &lt;- 27L class(goats) ## [1] &quot;integer&quot; Find the data type of the value \"school\" using the class() function. class(&quot;school&quot;) ## [1] &quot;character&quot; Assign your first name to a variable firstname and find its data type. Remember to enclose it in quotation marks firstname &lt;- &quot;Bryant&quot; # Any name will work class(firstname) ## [1] &quot;character&quot; Create a variable is_student and assign it the value TRUE. Use the class() function to find its data type. is_student &lt;- TRUE class(is_student) ## [1] &quot;logical&quot; ________________________________________________________________________________ 1.1.2 Data Structures This is the organization of data into or multiple data values in specific structures, they include vectors, matrix and data frames. Lets explore the mentioned data structures and their examples; Vector: This is a sequence of elements of the same data types(e.g., `c(1, 2, 3) is a numeric vector) Matrix: This is a two-dimensional data structure with rows and columns, where all elements are of the same type(e.g. numbers). Data Frames: This is the most common R data structure for handling tabular data(like an excel sheet). A data frame can contain different data types in each column unlike matrices and vectors. Data frames are central to real-world data analysis. You will work with them to analyze, transform, and visualize data sets, whether you are calculating averages or identifying trends. The is.vector, is.matrix and is.data.frame functions are used to confirm if the variable in question is a vector, matrix or data frame respectively. Examples Lets have some fun! We will create different data structures and find their data types: Create a vector, marks to store the values, 23, 67, 98, 34, 98, 21. Print the vector to the console and use is.vector function to confirm if its a actually a vector. marks = c(23, 67, 98, 34, 98, 21) print(marks) # print to the console ## [1] 23 67 98 34 98 21 is.vector(marks) # find its data structure ## [1] TRUE Create a matrix with values from 1 to 9 and use the is.matrix function to find to confirm if its really a matrix. vector1 = seq(1, 9) # Convert to matrix ## create by column m1=matrix(vector1, ncol=3) print(m1) # print the matrix to the console ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 is.matrix(m1) # confirms if its really a matrix ## [1] TRUE Create a data.frame from the above matrix. Add the column names as \"A\", \"B\", \"C\". Confirm if its really a matrix. var_names &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # vector to store variable names m1_df &lt;- data.frame(var_names, m1) # create the data frame print(m1_df) #print to the console ## var_names X1 X2 X3 ## 1 A 1 4 7 ## 2 B 2 5 8 ## 3 C 3 6 9 is.data.frame(m1_df) #confirms if its really a data.frame. ## [1] TRUE Practical Exercise Try out the exercise below to test your understanding in R data structures; Create a vector named height with the values 120.1, 118, 123.4, 130.8, 115.2. Use the is.vector to confirm that the created variable is a vector. Use length() function to count the number of elements in the vector. Create a matrix m1 from the vector v1 where v1 &lt;- seq(1, 12) with three columns. Use the is.matrix function to confirm if the said variable is a matrix. Access the third column by running the command m1[, 3]. Access the second row by running the command m1[2,]. Create a data frame students_df with the columns \"Name\", \"Age\", and \"Marks\" for three students. Where Name &lt;- c(\"Pragya\", \"Thomas\", \"Ali\"), Age &lt;- c(21, 19, 23) and Marks &lt;- c(68, 72, 67). Solution Create a vector named height with the values 120.1, 118, 123.4, 130.8, 115.2. Use the is.vector to confirm that the created variable is a vector. height &lt;- c(120.1, 118, 123.4, 130.8, 115.2) is.vector(height) ## [1] TRUE Use length() function to count the number of elements in the vector. length(height) # count the number of elements ## [1] 5 Create a matrix m1 from the vector v1 where v1 &lt;- seq(1, 12) with three columns. Use the is.matrix function to confirm if the said variable is a matrix. v1 &lt;- seq(1, 12) # Create vector v1 m1 &lt;- matrix(v1, ncol=3) # create a matrix from the vector m1 ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 is.matrix(m1) # Confirm if its a matrix ## [1] TRUE Access the third column by running the command m1[, 3]. m1[, 3] # Access the third columns ## [1] 9 10 11 12 Access the second row by running the command m1[2,]. m1[2,] # Access of the second row ## [1] 2 6 10 Create a data frame students_df with the columns \"Name\", \"Age\", and \"Marks\" for three students. Where Name &lt;- c(\"Pragya\", \"Thomas\", \"Ali\"), Age &lt;- c(21, 19, 23) and Marks &lt;- c(68, 72, 67). student_df &lt;- data.frame( &quot;Name&quot;= c(&quot;Pragya&quot;, &quot;Thomas&quot;, &quot;Ali&quot;), &quot;Age&quot;=c(21, 19, 23), &quot;Marks&quot;=c(68, 72, 67) ) student_df ## Name Age Marks ## 1 Pragya 21 68 ## 2 Thomas 19 72 ## 3 Ali 23 67 ________________________________________________________________________________ 1.2 Data Importing and Exporting Importing and exporting data is the foundation of data analysis workflows. The main two types of data files used are CSV and excel files. CSV Files: R can easily import CSV files using read.csv(\"filename.csv\"). The CSV is one of the most common formats you will encounter. Excel Files: For excel files, you can use the readxl package with the function read_excel. Try it: Let’s have some fun by importing; From CSV file m1_imported &lt;- read.csv(&quot;data/m1.csv&quot;) # import the csv data set m1_imported # Display the data ## X V1 V2 V3 ## 1 1 1 5 9 ## 2 2 2 6 10 ## 3 3 3 7 11 ## 4 4 4 8 12 From Excel file library(readxl) students_imported &lt;- read_excel(&quot;data/students.xlsx&quot;) # Import the data students_imported # Display the data ## # A tibble: 3 × 3 ## Name Age Marks ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pragya 21 68 ## 2 Thomas 19 72 ## 3 Ali 23 67 After data wrangling, manipulation and processing, the end product(processed data) can be saved for further use. The data can also be shared to others. Lets explore how export the CSV and Excel files To CSV: You can save your data to CSV format using write.csv(data, \"filepath.csv\"). To Excel To write to Excel, you can use write.xlsx(data, \"filepath.xlsx\") from the openxlsx package. Try it: Lets export the previously imported data set locally To CSV write.csv(m1_imported, &quot;data/m1_exported.csv&quot;) # Write the data set locally Excel File library(openxlsx) write.xlsx(students_imported, &quot;data/students_exported.xlsx&quot;) 1.3 Data Cleaning Before you analyze data, it is crucial to ensure that it is clean. Here are some common issues in data cleaning; Null Values: Missing data can distort your analysis. Functions like is.na() and na.omit() are used to detect and remove null values respectively. Null values can also be imputed by filling the missing values with the most relevant value for instance mean, mode or median of the variable, zero, or any dedicated value. Duplicated Records: Duplicates can cause bias in results and they can detected using the duplicated() function. This duplicated records can be removed by unique() function from R or distinct() from dplyr package. Outliers: These are extreme values that don’t follow the general trend. The use of summary statistics(specifically IQR) and boxplots can be used to cap these values based on the context. Data cleaning is like polishing a diamond-it ensures the data is ready for analysis, free from distortions like missing values or outliers that can skew your insights. Try it: Lets have some fan! We will create a random data set, identify all data issues and address them by cleaning. Create a sample data set # Create a dataset set.seed(42) df &lt;- data.frame( Product = c(&#39;Shoes&#39;, &#39;Laptop&#39;, &#39;Watch&#39;, &#39;Phone&#39;, &#39;Shoes&#39;, &#39;Watch&#39;, &#39;Laptop&#39;, &#39;Shoes&#39;, &#39;Laptop&#39;, &#39;Phone&#39;), Sales = c(150, 500, NA, 300, 150, 1000, 500, 150, 500, 300), # Outlier in Sales (1000) Category = c(&#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Fashion&#39;, &#39;Tech&#39;, &#39;Tech&#39;), Discount = c(10, 0, 20, 5, 10, 20, 0, 10, 0, 5), Returns = c(2, 0, 1, 0, 2, 1, 0, 2, 0, 0), Profit = c(30, 100, NA, 70, 30, 500, 100, 30, 100, 70) # Outlier in Profit (500) ) # Add duplicated rows df &lt;- rbind(df, df[2:3, ]) # View the dataset head(df) ## Product Sales Category Discount Returns Profit ## 1 Shoes 150 Fashion 10 2 30 ## 2 Laptop 500 Tech 0 0 100 ## 3 Watch NA Fashion 20 1 NA ## 4 Phone 300 Tech 5 0 70 ## 5 Shoes 150 Fashion 10 2 30 ## 6 Watch 1000 Fashion 20 1 500 Count the null values sum(is.na(df)) ## [1] 4 There are 4 null values in the data set. Let’s handle the null values by filling them with mean of the respective variables. # Fill missing Sales and Profit with the mean of the respective columns df$Sales[is.na(df$Sales)] &lt;- mean(df$Sales, na.rm = TRUE) df$Profit[is.na(df$Profit)] &lt;- mean(df$Profit, na.rm = TRUE) # View the data set after handling null values head(df) ## Product Sales Category Discount Returns Profit ## 1 Shoes 150 Fashion 10 2 30 ## 2 Laptop 500 Tech 0 0 100 ## 3 Watch 405 Fashion 20 1 113 ## 4 Phone 300 Tech 5 0 70 ## 5 Shoes 150 Fashion 10 2 30 ## 6 Watch 1000 Fashion 20 1 500 # Count the null values in the data set to confirm the operation sum(is.na(df)) ## [1] 0 The null values are now filled and the data set is complete. Let’s find if there exists some duplicated records and how many are they? # Count the duplicated rows sum(duplicated(df)) ## [1] 7 # Shape of the data set dim(df) ## [1] 12 6 There are 7 duplicated rows. We will remove the duplicated records and retain only one row of the same kind. This can be achieved using unique() from base R or distinct() from dplyr package. In this case we will be using the distinct command. # Load the required libraries library(dplyr) # Remove duplicated rows df_cleaned &lt;- df %&gt;% distinct() # Count the duplicated records sum(duplicated(df_cleaned)) ## [1] 0 # Shape of the data set dim(df_cleaned) ## [1] 5 6 The data has no duplicated records and it is evident that 7 records(duplicated) were deleted. They are only 5 rows remaining. The last step of data cleaning in this case is to identify outliers in the Sales and Profit, and remove them using the IQR method. # Use the IQR method to detect outliers in Sales and Profit Q1_sales &lt;- quantile(df_cleaned$Sales, 0.25) Q3_sales &lt;- quantile(df_cleaned$Sales, 0.75) IQR_sales &lt;- Q3_sales - Q1_sales Q1_profit &lt;- quantile(df_cleaned$Profit, 0.25) Q3_profit &lt;- quantile(df_cleaned$Profit, 0.75) IQR_profit &lt;- Q3_profit - Q1_profit # Filter out outliers df_cleaned &lt;- df_cleaned %&gt;% filter(!(Sales &lt; (Q1_sales - 1.5 * IQR_sales) | Sales &gt; (Q3_sales + 1.5 * IQR_sales))) %&gt;% filter(!(Profit &lt; (Q1_profit - 1.5 * IQR_profit) | Profit &gt; (Q3_profit + 1.5 * IQR_profit))) # Find out how many records were affected dim(df_cleaned) ## [1] 4 6 Only one row had outliers and was removed. The data is now clean and ready for further analysis. 1.4 Data Visualization Data visualization is the representation of data through use of common graphics, such as charts, plots, infographics and even animations. In this course we will use the famous ggplot2 library to create charts and graphs. ggplot is one of the most popular and flexible data visualization libraries in R. It follows the grammar of graphics philosophy, allowing you to build plots in layers. Here are some of the basic plots in data visualization; Scatter Plots: used to visualize the relationship between two variables in R. Bar Charts: used to compare categorical data. Histograms: used to represent distribution of a single continuous variable. Visualizations are powerful tools that help you see patterns and insights that raw data might hide. A well-made plot can communicate your findings more effectively than numbers alone. Try it: Lets use the above data set that we cleaned to plot simple charts in R using ggplot library. Install the package if not installed install.packages(&quot;ggplot2&quot;) Load the library library(ggplot2) There are 5 key steps in plotting in ggplot; The Setup - Read the data set, define x and y axis. ggplot(data, aes(x, y))+... The Labels - Title, X and Y axis labels. ... + labs(xlab=, ylab=, title=) + ... The Theme - Default, Black and White, colored etc. ....+ &lt;theme_type&gt;.. The Facets - Individual Graphs for each group in data with exactly same range The Layers or geoms - The actual plot type - e.g Bar plot, Box plot, Violin plot etc. ...+ geom_bar() + ... for bar chart, ...+ geom_point() + ... for scatter plot etc. Lets explore how to create a basic chart using ggplot2 library in R. We will create a sample student data set that we will use to visualize data in R. Create the data set library(dplyr) # Sample data for students set.seed(27) students_df &lt;- data.frame( student_id = 1:100, score = sample(50:100, 100, replace = TRUE), study_hours = sample(5:30, 100, replace = TRUE), gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 100, replace = TRUE), grade = sample(c(&quot;Freshman&quot;, &quot;Sophomore&quot;, &quot;Junior&quot;, &quot;Senior&quot;), 100, replace = TRUE) ) head(students_df) ## student_id score study_hours gender grade ## 1 1 54 10 Male Freshman ## 2 2 99 8 Male Junior ## 3 3 58 21 Male Senior ## 4 4 89 13 Male Junior ## 5 5 68 23 Female Freshman ## 6 6 65 10 Female Junior Plot the charts; Scatter plot We will plot to show the relationship between study hours and scores where scores is assumed to depend on study hours. Therefore scores will be on the y-axis and the study hours will be at the x-axis. # The set up ggplot(data = students_df, aes(x = study_hours, y = score)) + # The geoms - for scatter plot geom_point() + # Labels labs( title = &quot;Study hours vs scores&quot;, x = &quot;Weekly study hours&quot;, y = &quot;Scores&quot; ) + # Theme theme_classic() From the chart, study hours per week does not have an effect of the score - remember this is randomly generated data set. Bar Chart We will find the count of students in each grade level ggplot(data = students_df, aes(x=grade)) + geom_bar() + labs( title = &quot;Students by grade level&quot;, x = &quot;Grade level&quot;, y = &quot;Number of Students&quot; ) + theme_minimal() The Senior grade has the fewest students while the Sophomore grade takes the lead in the student population. Pie Chart We will plot the gender distribution of students on a pie chart to show the proportion of male and female students. # Load required libraries library(ggplot2) library(dplyr) # Ensure `gender` column is recognized students_df$gender &lt;- as.factor(students_df$gender) # Count gender distribution gender_count &lt;- dplyr::count(students_df, gender) # Create Pie Chart ggplot(data = gender_count, aes(x = &quot;&quot;, y = n, fill = gender)) + geom_bar(stat = &quot;identity&quot;, width = 1) + coord_polar(&quot;y&quot;) + labs( title = &quot;Gender Distribution of Students&quot;, fill = &quot;Gender&quot; ) + theme_classic() + theme( plot.title = element_text(hjust = 0.5), axis.title = element_blank(), axis.text = element_blank(), panel.grid = element_blank() ) From the pie chart, most students in the school are females. Histogram Finally we will plot a histogram to plot the distribution of scores. ggplot(data = students_df, aes(x=score)) + geom_histogram() + labs( title = &quot;Students&#39; scores distribution&quot;, x = &quot;Score&quot;, y = &quot;Count of students&quot; ) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 1.5 Hands-on Exercises You will be required to work with a randomly generated patients data. Here are the steps to take to work on this exercise; You will run the code below to generate the data set. # Set seed for reproducibility set.seed(71) # Generate the data set patients_data &lt;- data.frame( patient_id = 1:1000, age = sample(20:80, 1000, replace = TRUE), height_cm = rnorm(1000, mean = 165, sd=10), weight_kg = runif(1000, min=50, max=135), gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 1000, replace = TRUE), condition = sample(c(&quot;Hypertension&quot;, &quot;Malaria&quot;, &quot;Pneumonia&quot;, &quot;Diabetes&quot;, &quot;Asthma&quot;, &quot;Healthy&quot;), 1000, replace=TRUE) ) head(patients_data) ## patient_id age height_cm weight_kg gender condition ## 1 1 78 177.6077 108.48587 Female Diabetes ## 2 2 47 146.4456 130.03183 Female Healthy ## 3 3 34 158.7752 61.70929 Female Malaria ## 4 4 67 146.2848 114.83419 Female Asthma ## 5 5 65 146.6927 86.65565 Male Hypertension ## 6 6 20 155.5785 74.65025 Male Healthy You will use the patients_data to answer the questions below; Identify the data types for each column using the str() function. Find the shape of the data set using the dim function. Save the patients_data to CSV and name the file patients_data.csv. Import the saved data set as patients_df. Using the imported data set, patients_df, create the following charts. Plot a histogram to show the distribution of height. Create a bar chart to compare the number of male and female patients. Use a pie chart to show the composition of different health conditions. Plot a scatter plot to show the relationship between age and weight. Solution Run the code # Set seed for reproducibility set.seed(71) # Generate the data set patients_data &lt;- data.frame( patient_id = 1:1000, age = sample(20:80, 1000, replace = TRUE), height_cm = rnorm(1000, mean = 165, sd=10), weight_kg = runif(1000, min=50, max=135), gender = sample(c(&quot;Male&quot;, &quot;Female&quot;), 1000, replace = TRUE), condition = sample(c(&quot;Hypertension&quot;, &quot;Malaria&quot;, &quot;Pneumonia&quot;, &quot;Diabetes&quot;, &quot;Asthma&quot;, &quot;Healthy&quot;), 1000, replace=TRUE) ) head(patients_data) ## patient_id age height_cm weight_kg gender condition ## 1 1 78 177.6077 108.48587 Female Diabetes ## 2 2 47 146.4456 130.03183 Female Healthy ## 3 3 34 158.7752 61.70929 Female Malaria ## 4 4 67 146.2848 114.83419 Female Asthma ## 5 5 65 146.6927 86.65565 Male Hypertension ## 6 6 20 155.5785 74.65025 Male Healthy You will use the patients_data to answer the questions below; Identify the data types for each column using the str() function. str(patients_data) ## &#39;data.frame&#39;: 1000 obs. of 6 variables: ## $ patient_id: int 1 2 3 4 5 6 7 8 9 10 ... ## $ age : int 78 47 34 67 65 20 58 69 27 31 ... ## $ height_cm : num 178 146 159 146 147 ... ## $ weight_kg : num 108.5 130 61.7 114.8 86.7 ... ## $ gender : chr &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ condition : chr &quot;Diabetes&quot; &quot;Healthy&quot; &quot;Malaria&quot; &quot;Asthma&quot; ... Find the shape of the data set using the dim function. dim(patients_data) ## [1] 1000 6 Save the patients_data to CSV and name the file patients_data.csv. write.csv(patients_data, &quot;data/patients_data.csv&quot;) Import the saved data set as patients_df. patients_df &lt;- read.csv(&quot;data/patients_data.csv&quot;) head(patients_df) ## X patient_id age height_cm weight_kg gender condition ## 1 1 1 78 177.6077 108.48587 Female Diabetes ## 2 2 2 47 146.4456 130.03183 Female Healthy ## 3 3 3 34 158.7752 61.70929 Female Malaria ## 4 4 4 67 146.2848 114.83419 Female Asthma ## 5 5 5 65 146.6927 86.65565 Male Hypertension ## 6 6 6 20 155.5785 74.65025 Male Healthy Using the imported data set, patients_df, create the following charts. Plot a histogram to show the distribution of height. Create a bar chart to compare the number of male and female patients. Use a pie chart to show the composition of different health conditions. Plot a scatter plot to show the relationship between age and weight. library(ggplot2) # Histogram ggplot(patients_df, aes(x = height_cm)) + geom_histogram(fill=&quot;skyblue&quot;, color=&quot;black&quot;) + labs( title = &quot;Distribution of height&quot;, y = &quot;Frequency&quot;, x = &quot;Height(cm)&quot; ) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The distribution of height follows a near normal distribution. Plotting a bar chart ggplot(patients_df, aes(x = gender, fill=gender)) + geom_bar() + labs( title = &quot;Gender distribution&quot;, x = &quot;Gender&quot;, y = &quot;Count&quot; ) + theme_minimal() The female gender had more patients than the male patients Plot a pie chart library(dplyr) # Ensure `condition` column is recognized patients_df$condition &lt;- as.factor(patients_df$condition) # Find the condition count condition_count &lt;- patients_df %&gt;% dplyr::count(condition) # Plot the data ggplot(condition_count, aes(x=&quot;&quot;, y=n, fill = condition)) + geom_bar(stat = &quot;identity&quot;, width=1) + coord_polar(&quot;y&quot;) + labs( title = &quot;Proportion of Patients by Conditions&quot; ) + theme_classic() Plot the relationship between age and weight ggplot(patients_df, aes(x = age, y = weight_kg)) + geom_point() + labs( title = &quot;Age vs Weight&quot;, x = &quot;Age&quot;, y = &quot;Weight&quot; ) + theme_minimal() ________________________________________________________________________________ "],["functions.html", "Chapter 2 Functions 2.1 Writing Functions 2.2 Calling the Functions 2.3 Function Documentation 2.4 Hands-on Exercise", " Chapter 2 Functions In programming, functions are like little blocks of code that perform a specific task. Think of them as reusable instructions that you can call whenever you need them. Here’s why functions are super helpful: Avoid repetition: Instead of writing the same code multiple times, you can just call the function. Cleaner code: Your code becomes easier to read and maintain because functions help organize it better. Easier debugging: When something goes wrong, you only need to check the function itself rather than searching through your entire program. Why Use Functions? Imagine having to rewrite a set of instructions every time you need them! With functions, you write the code once and reuse it as many times as you want. A good rule of thumb is: if you expect to run a specific set of instructions more than twice, create a function for it. What Can Functions Do? Functions are flexible and can be used for many different purposes: Take input (called arguments) Process the input based on what the function is meant to do Return a result after completing the task 2.1 Writing Functions Lets take a tour on different types of functions in R before diving deep into writing functions. This will help you understand when to write functions and when to use readily-available functions. There are three main types of functions: User-Defined Functions (UDF) – Custom functions you write for your specific needs. Built-in functions – These come pre-loaded in R. Example: mean() Package functions – Functions from external R packages you can install. Example: ggplot() and select() from ggplot2 and dplyr respectively. 2.1.1 User-Defined Functions The best way to grasp how functions work in R is by creating your own! These are called* User-Defined Functions (UDFs), and they allow you to design custom tasks that fit your needs. In R, functions typically follow this format: function_name &lt;- function(argument_1, argument_2) { # Function body (your instructions go here) return(output) } Let’s break down the key elements: Function Name: This is how you’ll call your function later. When you create a function, you assign it a name and save it as a new object. For example, if you name your function calculate_mean, that’s the name you’ll use every time you want to run the function. Arguments (also called Parameters): Arguments are placed inside the parentheses. They tell the function what input to expect or how to modify its behavior. Think of them as placeholders for the data you’ll provide later when you run the function. Function Body: Inside the curly brackets {}, you’ll write the instructions that the function will follow to accomplish the task. This is the “heart” of the function. Return Statement: The return() function tells R what result to give you after the function finishes its job. It’s optional, but it helps if you want to store the function’s result in a variable. Let’s write a simple function that calculates the mean (average) of two numbers: mean_two_numbers &lt;- function(num_1, num_2) { mean &lt;- (num_1 + num_2) / 2 return(mean) } How to Use the Function: To find the mean of 10 and 20, simply call the function like this: mean_two_numbers(10, 20) ## [1] 15 Let’s add a few more simple tasks: writing a function that calculates the difference between two numbers. Why is this important? Well, imagine you have two values and you want to find their difference—that’s exactly what this function will help us do! # Function to calculate the difference between two numbers calculate_difference &lt;- function(x, y) { # Subtract the second number (y) from the first number (x) difference &lt;- x - y # Return the difference result so we can use it later return(difference) } You see!: x and y are our arguments: These are the two numbers we’ll use in our calculation. The subtraction happens inside the function: We simply subtract y from x and store the result in difference. Finally, we return the difference: This way, we can use the result when we call the function. Now, let’s put it to the test! We’ll run the function with different sets of numbers and see what we get: calculate_difference(10, 5) # 10 - 5 = 5 ## [1] 5 calculate_difference(25, 15) # 25 - 15 = 10 ## [1] 10 calculate_difference(50, 30) # 50 - 30 = 20 ## [1] 20 Notice how easy it is to calculate the difference between any two numbers by just calling our function? That’s the power of writing your own functions—they make life a lot easier! Now, lets make it more interesting! How about a function that greets you by name? We can do the same in R by creating a simple function that takes someone’s name and returns a greeting. Here is how we do it: # Function to greet a student by their name greet_student &lt;- function(student_name) { # Create a personalized greeting greeting &lt;- paste(&quot;Hello&quot;, student_name, &quot;!&quot;) # Return the greeting so we can use it later return(greeting) } Remember! We use student_name as the argument: This is where you pass in the name of the student. We combine \"Hello\" with the name: The paste() function(that is an -in-built function which will discuss later in the course) helps us put the pieces together to form a full sentence. Return the greeting: The function gives us back a customized message, ready to greet anyone! Lets try it out with different names greet_student(&quot;John&quot;) # Hello John! ## [1] &quot;Hello John !&quot; greet_student(&quot;Alice&quot;) # Hello Alice! ## [1] &quot;Hello Alice !&quot; greet_student(&quot;Michael&quot;) # Hello Michael! ## [1] &quot;Hello Michael !&quot; Remember to try it out with your name! Key Takeaways: By writing these two simple functions, you’ve already tackled a lot of important concepts in R! You now know: How to create a function. How to pass arguments (inputs/parameters) to a function. How to return a result that you can use later. Practical Exercise In this exercise, you’ll get hands-on practice creating your own functions in R. Follow the instructions below to write functions that perform specific tasks. Remember to test your functions with different input values! Create a function called add_numbers that takes two arguments, a and b, and returns their sum. Write a function named is_even that takes a single argument, num, and returns \"Even\" if the number is even, or \"Odd\" if it’s odd. Create a function called find_max that takes three arguments and returns the largest of the three numbers. Solution Create a function called add_numbers that takes two arguments, a and b, and returns their sum. # Function to calculate the sum of two numbers sum_two_numbers &lt;- function(x, y) { sum &lt;- x + y return(sum) } # Test the function with different values sum_two_numbers(5, 10) # Output: 15 ## [1] 15 sum_two_numbers(20, 30) # Output: 50 ## [1] 50 sum_two_numbers(100, 200) # Output: 300 ## [1] 300 Write a function named is_even that takes a single argument, num, and returns \"Even\" if the number is even, or \"Odd\" if it’s odd. # Function to check if a number is even or odd check_even_odd &lt;- function(number) { if (number %% 2 == 0) { return(&quot;Even&quot;) } else { return(&quot;Odd&quot;) } } # Test the function with different numbers check_even_odd(4) # Output: &quot;Even&quot; ## [1] &quot;Even&quot; check_even_odd(7) # Output: &quot;Odd&quot; ## [1] &quot;Odd&quot; check_even_odd(10) # Output: &quot;Even&quot; ## [1] &quot;Even&quot; Create a function called find_max that takes three arguments and returns the largest of the three numbers. # Function to find the maximum of three numbers max_of_three &lt;- function(a, b, c) { max_value &lt;- max(a, b, c) # Use the built-in max function return(max_value) # Return the maximum value } # Test the function with different values max_of_three(10, 20, 5) # Output: 20 ## [1] 20 max_of_three(3, 1, 2) # Output: 3 ## [1] 3 max_of_three(7, 15, 12) # Output: 15 ## [1] 15 ________________________________________________________________________________ 2.1.2 Built-in Fuctions We have learned how to create our own user-defined functions (UDFs) to perform specific tasks. Now, let’s dive deeper into R’s capabilities by exploring its built-in functions. These handy tools are readily available for you to use anytime, making your coding experience even smoother. R is packed with a treasure trove of built-in functions that allow you to perform a variety of tasks with just a few simple commands. Whether you’re crunching numbers or analyzing data, these functions are your best friends. Here’s a sneak peek at some of the most useful built-in functions in R: print(): This function displays an R object right on your console. It’s like saying, “Hey, look at this!” print(&quot;Hello Mum&quot;) ## [1] &quot;Hello Mum&quot; min() and max(): Need to find the smallest or largest number in a bunch? These functions will do just that for a numeric vector. sum(): Want to add up a series of numbers? Use sum() to get the total of a numeric vector. mean(): This function calculates the average of your numbers. Perfect for when you need to find the middle ground! range(): Curious about the minimum and maximum values of your numeric vector? range() has you covered. str(): Want to understand the structure of an R object? str() will give you a clear picture of what’s inside. ncol(): If you’re working with matrices or data frames, this function tells you how many columns you have. length(): This one returns the number of items in an R object, whether it’s a vector, a list, or a matrix. Here’s a quick example to show you how easy it is to use these functions with a vector of numbers: v &lt;- c(1, 3, 0.2, 1.5, 1.7) # Create a vector print(v) # Display the vector ## [1] 1.0 3.0 0.2 1.5 1.7 sum(v) # Calculate the total sum ## [1] 7.4 mean(v) # Find the average ## [1] 1.48 length(v) # Get the number of elements ## [1] 5 As you can see, working with R’s built-in functions is straightforward and super helpful. Start experimenting with these functions and watch how they can simplify your coding experience! Key Takeaways: By completing this exercise, you’ve already tackled several important concepts in R! You now know: How to create a vector and use it for calculations. How to utilize built-in functions like sum(), max(), min(), mean(), and length(). How to derive meaningful statistics from data using R’s built-in capabilities R has a wealth of resources on this topic, and as you gain more experience and knowledge, you’ll uncover even more advanced built-in functions that can simplify your programming tasks. Practical Exercise In this exercise, you are required to create a vector named numbers that contains the following values: 4, 8, 15, 16, 23, 42. After creating the vector, you will use various built-in functions to analyze it based on the instructions below; Use the sum() function to calculate the total of the numbers vector. Use the max() function to find the maximum value in the numbers vector. Use the min() function to find the minimum value in the numbers vector. Use the mean() function to calculate the average of the numbers vector. Use the length() function to find out how many elements are in your numbers vector. Solution In this exercise, you are required to create a vector named numbers that contains the following values: 4, 8, 15, 16, 23, 42. After creating the vector, you will use various built-in functions to analyze it based on the instructions below; # Create a vector numbers &lt;- c(4, 8, 15, 16, 23, 42) Use the sum() function to calculate the total of the numbers vector. sum(numbers) ## [1] 108 Use the max() function to find the maximum value in the numbers vector. max(numbers) ## [1] 42 Use the min() function to find the minimum value in the numbers vector. min(numbers) ## [1] 4 Use the mean() function to calculate the average of the numbers vector. mean(numbers) ## [1] 18 Use the length() function to find out how many elements are in your numbers vector. length(numbers) ## [1] 6 ________________________________________________________________________________ 2.1.3 Package Functions Just like we’ve learned about User-Defined and Built-in Functions, R also provides a vast number of additional functions through packages. These packages extend R’s capabilities and allow you to perform specific tasks, from data manipulation to machine learning, with ease. What are R Package Functions? Packages in R are collections of R functions, data, and compiled code that are stored in a well-defined format. While R comes with a set of built-in functions, packages allow you to go beyond the basic functionality. You can install and load packages based on the task you want to accomplish. Think of package functions as tools in a toolbox: not everything is built-in, but by adding specific tools, you can perform new tasks easily. Lets explore how to get started using the functions; Installing and Loading Packages To use functions from a package, you first need to install the package and load it into your R session. install.packages(&quot;package_name&quot;) Every time you start a new R session if you want to use the functions from that package. Load the package by; library(package_name) To put this into real-life action, let’s learn about the dplyr package, which is commonly used for data manipulation. It contains many useful functions to work with data frames or tibbles (a modern version of data frames). Here’s an example of how to install and load dplyr, and use some of its core functions. Install the package install.packages(&quot;dplyr&quot;) # Install it once Load the package library(dplyr) # Load it whenever you need to use it Let’s explore a few package functions from dplyr: select(): Chooses specific columns from a dataset. filter(): Filters rows based on conditions. mutate(): Adds new variables (columns) or modifies existing ones. summarise(): Summarizes data, such as calculating the mean or total We will create a data frame to demonstrate how to use functions from the dplyr package. # Create a data frame for demonstration data &lt;- data.frame( Name = c(&quot;John&quot;, &quot;Jane&quot;, &quot;David&quot;, &quot;Anna&quot;), Age = c(28, 34, 22, 19), Score = c(85, 90, 88, 92) ) # 1. Select only the Name and Score columns selected_data &lt;- dplyr::select(data, Name, Score) selected_data ## Name Score ## 1 John 85 ## 2 Jane 90 ## 3 David 88 ## 4 Anna 92 # 2. Filter rows where Score is greater than 88 filtered_data &lt;- filter(data, Score &gt; 88) filtered_data ## Name Age Score ## 1 Jane 34 90 ## 2 Anna 19 92 # 3. Add a new column that increases Score by 10 mutated_data &lt;- mutate(data, New_Score = Score + 10) mutated_data ## Name Age Score New_Score ## 1 John 28 85 95 ## 2 Jane 34 90 100 ## 3 David 22 88 98 ## 4 Anna 19 92 102 # 4. Calculate the average age summary_data &lt;- summarise(data, Average_Age = mean(Age)) summary_data ## Average_Age ## 1 25.75 In this example, we used functions from the dplyr package to select columns, filter rows, modify data, and summarize it! Key Takeaways: By learning about R package functions, you’ve unlocked even more tools to work efficiently in R. Here’s what you’ve learned today: How to install and load R packages. How to use package functions like those in dplyr for data manipulation. How to perform tasks like selecting columns, filtering data, and summarizing values. Packages in R allow you to extend the functionality of the base language for specific tasks. With packages, R becomes an even more powerful tool, allowing you to work with more advanced data sets and perform complex operations with ease! Practical Exercise In this exercise, you will use the functions from the dplyr package to manipulate the iris data set. Remember the dplyr package is installed by: install.packages(&quot;dplyr&quot;) and is loaded by: library(dplyr) The iris data set is loaded by data(&quot;iris&quot;) # view the first few columns head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Solve the following questions; Use the select function to select the Sepal.Length, Sepal.Width, and Species columns. Use the filter function to filter rows where Sepal.Length is greater than 5. Use the mutate function to create a new column Sepal.Ratio that divides Sepal.Length by Sepal.Width. Solution In this exercise, you will use the functions from the dplyr package to manipulate the iris data set. Remember the dplyr package is installed by: install.packages(&quot;dplyr&quot;) and is loaded by: library(dplyr) The iris data set is loaded by data(&quot;iris&quot;) # view the first few columns head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Solve the following questions; Use the select function to select the Sepal.Length, Sepal.Width, and Species columns. selected_iris &lt;- dplyr::select(iris, Sepal.Length, Sepal.Width, Species) head(selected_iris) ## Sepal.Length Sepal.Width Species ## 1 5.1 3.5 setosa ## 2 4.9 3.0 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5.0 3.6 setosa ## 6 5.4 3.9 setosa Use the filter function to filter rows where Sepal.Length is greater than 5. filtered_iris &lt;- dplyr::filter(iris, Sepal.Length&gt;5) head(filtered_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 5.4 3.9 1.7 0.4 setosa ## 3 5.4 3.7 1.5 0.2 setosa ## 4 5.8 4.0 1.2 0.2 setosa ## 5 5.7 4.4 1.5 0.4 setosa ## 6 5.4 3.9 1.3 0.4 setosa Use the mutate function to create a new column Sepal.Ratio that divides Sepal.Length by Sepal.Width. updated_iris &lt;- dplyr::mutate(iris, Sepal.Ratio=Sepal.Length/Sepal.Width) head(updated_iris$Sepal.Ratio) ## [1] 1.457143 1.633333 1.468750 1.483871 1.388889 1.384615 ________________________________________________________________________________ 2.1.4 Type of arguments in R functions Now that we’ve learned and explored different types of functions, let’s dive into function arguments to strengthen your understanding of writing functions. Arguments are essential components of any function. Although it’s possible to write a function without parameters, like the example below, most functions do require arguments to tell them what data to process hello &lt;- function() { print(&#39;Hello, my friend&#39;) } Why Arguments Matter Arguments are the input for functions. They allow us to give the function specific values to work with. If we want a function to handle different cases or data, arguments give us that flexibility. When defining arguments, you include them inside the parentheses of the function definition, separated by commas. Generally, functions with more arguments tend to be more complex, but they also offer greater control over what the function does. # Creating a function with arguments my_function &lt;- function(argument1, argument2){ # function body } Handling Missing Arguments Whenever you create a function with parameters, you must provide the values for those parameters when calling the function. Otherwise, R will return an error. For example, if you forget to supply both numbers in a function to calculate their mean, the function won’t work. But you can avoid this issue by using default arguments. These are preset values that the function will use if you don’t provide them during the call. Let’s modify our mean function to demonstrate: mean_two_numbers &lt;- function(num_1, num_2 = 30) { mean &lt;- (num_1 + num_2) / 2 return (mean) } In this version, if you only provide one value when calling the function, R will automatically use the default value for the second number (which is 30 in this case): mean_two_numbers(num_1 = 10) ## [1] 20 You now understand how arguments work and the importance of default values in making functions more flexible and error-proof. Practical Exercise Create a function greet that prints a simple message like \"Hello, welcome to R programming!\". Write a function multiply_numbers that takes two arguments, a and b, and returns the product of these numbers Create a function calculate_total that accepts two arguments, price and tax_rate. Set a default value of tax_rate = 0.15 (15%). Solution Create a function greet that prints a simple message like \"Hello, welcome to R programming!\". greet &lt;- function() { print(&quot;Hello, welcome to R programming!&quot;) } # Call the function greet() ## [1] &quot;Hello, welcome to R programming!&quot; Write a function multiply_numbers that takes two arguments, a and b, and returns the product of these numbers. multiply_numbers &lt;- function(a, b) { return(a * b) } # Call the function multiply_numbers(6, 8) ## [1] 48 Create a function calculate_total that accepts two arguments, price and tax_rate. Set a default value of tax_rate = 0.15 (15%). calculate_total &lt;- function(price, tax_rate = 0.15) { total &lt;- price + (price * tax_rate) return(total) } # Call the function calculate_total(price=160) ## [1] 184 ________________________________________________________________________________ 2.1.5 Understanding Return Values in R Functions In many programming languages, functions take data as input and produce some result as output. Often, you must use a return statement to explicitly give back the result. Otherwise, the value might only be visible inside the function and not available to use later. But in R, the situation is a little more relaxed! In R, a function will always return a value that can be stored in a variable, even without a return statement. However, for clarity and good practice, it’s still helpful to include return to show your intent. Let’s walk through an example: mean_sum &lt;- function(num_1, num_2) { mean &lt;- (num_1 + num_2) / 2 sum &lt;- num_1 + num_2 return(list(mean = mean, sum = sum)) } Now, calling the function: results &lt;- mean_sum(10, 20) print(results) # You&#39;ll see both the mean and sum printed ## $mean ## [1] 15 ## ## $sum ## [1] 30 2.2 Calling the Functions In previous sections, we’ve seen how to call functions with different arguments. Now, let’s dig a little deeper into how R works behind the scenes when you pass arguments to a function. R allows two main ways of passing arguments: By position – The arguments are passed in the same order as the function definition. By name – You explicitly mention the argument name and its value. You can also mix these two strategies! Let’s explore these options using an example. Here’s a simple function that takes two arguments: name and surname. hello &lt;- function(name, surname) { print(paste(&#39;Hello&#39;, name, surname)) } Lets call the function using different strategies; By Position You pass the arguments in the exact order the function expects. hello(&#39;Jane&#39;, &#39;McCain&#39;) ## [1] &quot;Hello Jane McCain&quot; By Name When using this method, the order doesn’t matter. You just specify the argument names. hello(surname = &#39;McCain&#39;, name = &#39;Jane&#39;) ## [1] &quot;Hello Jane McCain&quot; Mixing Position and Name You can mix both approaches. Named arguments are matched first, then the remaining ones are matched by position hello(surname = &#39;McCain&#39;, &#39;Jane&#39;) ## [1] &quot;Hello Jane McCain&quot; This flexibility can make your code easier to read and maintain, especially when functions have many arguments! 2.3 Function Documentation Finally when writing functions, it’s always a good idea to provide documentation to guide users on how to use the function. This is especially important when dealing with complex functions or when the function is shared with others. One simple way to add documentation is by including comments in the body of your function. These comments explain what each part of the function does. This is an informal method, but it helps both you and others quickly understand what’s happening in the function. Here’s an example: hello &lt;- function(name, surname) { # Say hello to a person with their name and surname print(paste(&#39;Hello,&#39;, name, surname)) } If you call the function without executing it, you’ll see its structure along with the comments: hello ## function(name, surname) { ## # Say hello to a person with their name and surname ## print(paste(&#39;Hello,&#39;, name, surname)) ## } If your function is part of a larger package and you want it to be properly documented, you should write formal documentation in a separate .Rd file. These files store structured documentation, which you can access using ?function_name in R, similar to the help file you see for built-in functions like ?mean. Formal documentation includes details such as: Function name and description. Arguments and their roles. Examples of how to use the function. Output that the function returns. This approach ensures that users can easily understand and use your function, even in complex packages. 2.4 Hands-on Exercise You will attempt this hands-on exercise to confirm your understanding of functions. For one of the functions you created, add comments inside the function to explain what each part of the function does. Create a User-Defined Function (UDF) named calculate_area that takes two arguments: length and width. The function should return the area of a rectangle. Create a vector named values with the numbers 4, 8, 15, 16, 23, 42. Use the built-in sum() function to calculate the total of the values vector and print the result. Write a function named greet that takes one argument, student_name, and prints a greeting. Modify the function to have a default argument that greets a \"Student\" if no name is provided. Create a function named mean_and_median that takes a numeric vector as an argument and returns both the mean and median of that vector as a list. Solution Create a User-Defined Function (UDF) named calculate_area that takes two arguments: length and width. The function should return the area of a rectangle. calculate_area &lt;- function(length, width) { # Calculate the area by multiplying length and width area &lt;- length * width # Return the calculated area return(area) } # Example usage of the calculate_area function area_result &lt;- calculate_area(5, 10) # Length: 5, Width: 10 print(paste(&quot;Area of rectangle:&quot;, area_result)) ## [1] &quot;Area of rectangle: 50&quot; Create a vector named values with the numbers 4, 8, 15, 16, 23, 42. Use the built-in sum() function to calculate the total of the values vector and print the result. values &lt;- c(4, 8, 15, 16, 23, 42) # Use the built-in sum() function to calculate the total of the values vector total &lt;- sum(values) # Print the result print(paste(&quot;Total of values vector:&quot;, total)) ## [1] &quot;Total of values vector: 108&quot; Write a function named greet that takes one argument, student_name, and prints a greeting. Modify the function to have a default argument that greets a \"Student\" if no name is provided. greet &lt;- function(student_name = &quot;Student&quot;) { # Print a greeting using the provided name or default to &quot;Student&quot; print(paste(&quot;Hello,&quot;, student_name)) } # Example usage of the greet function with a provided name greet(&quot;John&quot;) # Should print &quot;Hello, John&quot; ## [1] &quot;Hello, John&quot; # Example usage of the greet function without providing a name greet() ## [1] &quot;Hello, Student&quot; Create a function named mean_and_median that takes a numeric vector as an argument and returns both the mean and median of that vector as a list. mean_and_median &lt;- function(num_vector) { # Calculate the mean of the vector mean_value &lt;- mean(num_vector) # Calculate the median of the vector median_value &lt;- median(num_vector) # Return both mean and median as a list return(list(mean = mean_value, median = median_value)) } # Example usage of the mean_and_median function results &lt;- mean_and_median(c(12, 19, 21, 14, 09)) # Print the results print(paste(&quot;Mean:&quot;, results$mean, &quot;, Median:&quot;, results$median)) ## [1] &quot;Mean: 15 , Median: 14&quot; ________________________________________________________________________________ "],["group-manipulation.html", "Chapter 3 Group Manipulation 3.1 Apply Family 3.2 Aggregate Plyr 3.3 Data Reshaping 3.4 Hands-on Exercise", " Chapter 3 Group Manipulation Group manipulation in R refers to the process of grouping data based on certain categories and then performing operations based on each group separately. This is useful when you want to summarize, analyze or transform subsets of your data independently. In simple terms, group manipulation involves splitting the data into groups, applying a function to each group, and then combining the results. We will explore different methods designed by researchers for group manipulation. They are group manipulation using; The apply family, The aggregate from plyr package, Data reshaping 3.1 Apply Family The apply family in R is a collection of functions that helps you apply operations to data structures like vectors, lists, matrices and data frames in a more efficient way than using loops. Think of these functions as a way to give commands to your data in bulk, telling each piece what to do without repeating yourself. Let’s make this fun! Imagine you’re running a café, and you have tables (rows of data) with customer orders (columns of data). You want to calculate the total for each table or find out how much each customer spent on average. The apply family is like hiring a helper who goes to each table and collects information without you having to ask each customer individually! We will have a quick overview of the members of the apply family; apply() - Works with matrices or data frames, applying a function to rows or columns. lapply() - Loops over elements in a list, applying a function to each element and returning a list. sapply() - Similar to lapply, but it returns a vector or matrix when possible. tapply() - Applies a function over subsets of data, especially useful for factors or groups. mapply() - Applies a function to multiple arguments simultaneously. Try it: Here is the apply family in action using the built-in R data set that contains information about flowers. Use apply to calculate the mean of each column in the iris data set at once(No need of specifying the columns) # Load and view the first few rows of the iris data set data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # Calculate the mean of each numeric column col_means &lt;- apply(iris[, 1:4], 2, mean) print(col_means) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 The 2 in apply means “apply the function to columns” and the mean was used to find the average of each column. This is simple as asking a helper to calculate the the average for all types of flowers for each characteristic (sepal length, petal length, etc.). Let’s repeat the same for a each row, instead of argument value 2 we will put argument value 1 in the second position. row_means &lt;- apply(iris[, 1:4], 1, mean) # Calculate the mean for each row head(row_means, 15) # Show the first fifteen averages of the row ## [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 2.325 ## [14] 2.125 2.800 Now lets use the lapply function to find the range for each numeric column. This function applies to each element and returns a list. No need to specify if its a column or a row # Calculate the range of each numeric column in the iris dataset column_ranges &lt;- lapply(iris[, 1:4], range) print(column_ranges) ## $Sepal.Length ## [1] 4.3 7.9 ## ## $Sepal.Width ## [1] 2.0 4.4 ## ## $Petal.Length ## [1] 1.0 6.9 ## ## $Petal.Width ## [1] 0.1 2.5 Repeating the function with mean function instead of the range function. # Calculate the mean of each numeric column in the iris dataset col_means &lt;- lapply(iris[, 1:4], mean) print(col_means) ## $Sepal.Length ## [1] 5.843333 ## ## $Sepal.Width ## [1] 3.057333 ## ## $Petal.Length ## [1] 3.758 ## ## $Petal.Width ## [1] 1.199333 You see! lapply function works column wise instead of row wise when working with data frames. Lets create a function that will add 10 to the input value and use the lapply function to work on a vector. # Create a vector current_ages &lt;- c(21, 43, 12, 56, 32) # Create a function that adds 10 to an input value add_10 &lt;- function(value){ return(value + 10) } # Test the function add_10(27) ## [1] 37 # Apply the function to vector ages ages_10_years_later &lt;- lapply(current_ages, add_10) ages_10_years_later # Show the result ## [[1]] ## [1] 31 ## ## [[2]] ## [1] 53 ## ## [[3]] ## [1] 22 ## ## [[4]] ## [1] 66 ## ## [[5]] ## [1] 42 It returns a list with values in the vector current_ages add 10 to each value. The sapply() function works similarly to lapply(), but it tries to simplify the output. If possible, it will return a vector or matrix instead of a list. Let`s calculate the variance for each numeric column; # Calculate the variance for each numeric column col_variance &lt;- sapply(iris[, 1:4], var) print(col_variance) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.6856935 0.1899794 3.1162779 0.5810063 Remember that we created a function add_10 that adds 10 to the current ages of the clients. Lets repeat the same using the sapply function instead of lapply function. # Calculate the variance for each numeric column ages_10_years_later &lt;- sapply(current_ages, add_10) print(ages_10_years_later) ## [1] 31 53 22 66 42 It is now evident that sapply has a simpler output than the lapply function. The tapply() function applies a function to subsets of data grouped by a factor (e.g., species in our case). Let’s calculate the average sepal length for each species: # Calculate the average Sepal.Length for each Species avg_sepal_by_species &lt;- tapply(iris$Sepal.Length, iris$Species, mean) print(avg_sepal_by_species) ## setosa versicolor virginica ## 5.006 5.936 6.588 This is like sending your helper to collect the sepal lengths for each species separately, and then calculating the average for each group. Finally the mapply() function is useful when you want to apply a function to multiple sets of arguments at once. Let’s calculate the sum of Sepal.Length and Sepal.Width for each row: # Sum Sepal.Length and Sepal.Width for each row sepal_sum &lt;- mapply(sum, iris$Sepal.Length, iris$Sepal.Width) head(sepal_sum) ## [1] 8.6 7.9 7.9 7.7 8.6 9.3 This function adds the sepal length and width for each flower row by row. It’s like your helper asking every customer for two values and summing them up together. Practical Exercise Now it’s time to test your skills! Use apply() to calculate the maximum for each column in the iris data set. Use lapply() to find the summary statistics (use the summary() function) for each numeric column in the iris data set. Use tapply() to find the average petal width for each species in the iris data set. Solution Use apply() to calculate the maximum for each column in the iris data set. max_values &lt;- apply(iris[, 1:4], 2, max) print(max_values) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 7.9 4.4 6.9 2.5 Use lapply() to find the summary statistics (use the summary() function) for each numeric column in the iris data set. sum_stats &lt;- lapply(iris[,1:4], summary) print(sum_stats) ## $Sepal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Sepal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.800 3.000 3.057 3.300 4.400 ## ## $Petal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 ## ## $Petal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.100 0.300 1.300 1.199 1.800 2.500 Use tapply() to find the average petal width for each species in the iris data set. # Calculate the average Petal.Width for each Species avg_petal_width_by_species &lt;- tapply(iris$Petal.Width, iris$Species, mean) print(avg_petal_width_by_species) ## setosa versicolor virginica ## 0.246 1.326 2.026 ________________________________________________________________________________ 3.2 Aggregate Plyr The aggregate() function from plyr package is a powerful tool for grouping and summarizing data in R. This is similar to the SQL GROUP BY command or the tapply() that we have discussed above. The difference is that aggregate() allows to summarize data based on one or more grouping factors. Try it! Let’s explore an example using the built-in mtcars data set to show how to use the aggregate() from the plyr package. The plyr package can be installed by: install.packages(&quot;plyr&quot;) Lets start library(plyr) # Load the data set data(&quot;mtcars&quot;) # Use aggregate to find the average &#39;mpg&#39; (miles per gallon) grouped by the number of cylinders (&#39;cyl&#39;) avg_mpg_by_cyl &lt;- aggregate(mpg ~ cyl, data = mtcars, FUN = mean) avg_mpg_by_cyl ## cyl mpg ## 1 4 26.66364 ## 2 6 19.74286 ## 3 8 15.10000 If we break done the code; mpg ~ cyl tells R to calculate the average mpg(dependent variable) for each unique value of cyl(grouping factor). data = mtcars specifies the data set. FUN = mean applies the mean function to compute the average mpg for each group of cyl. We have just calculated the average mpg (miles per gallon) grouped by the number of cyl(cylinders). Let’s make it a little bit more complex by grouping with multiple variables and summarize multiple columns as well. We will calculate the mean horsepower(hp) and the weight(wt) by the number of cylinders(cyl) and the number of transmission(am). # Use aggregate to find the mean hp and wt by cylinders and transmission type avg_hp_wt_by_cyl_am &lt;- aggregate(cbind(hp, wt) ~ cyl + am, data = mtcars, FUN = mean) avg_hp_wt_by_cyl_am ## cyl am hp wt ## 1 4 0 84.66667 2.935000 ## 2 6 0 115.25000 3.388750 ## 3 8 0 194.16667 4.104083 ## 4 4 1 81.87500 2.042250 ## 5 6 1 131.66667 2.755000 ## 6 8 1 299.50000 3.370000 If we breakdown the code; cbind(hp, wt) allows you to summarize multiple columns (hp and wt). cyl + am groups the data by the number of cylinders and the transmission type (am = 0 for automatic, 1 for manual`). The argument FUN defines the function to be used here therefore, FUN = mean calculates the mean values for hp and wt for each group of cyl and am. Practical Exercise Try using the aggregate() with the iris data set to find the mean sepal length (Sepal.Length) and petal length(Petal.Length) for each species. Solution library(plyr) # Load the iris data set data(iris) # Calculate the averages as per the instructions avg_sepal_petal_by_species &lt;- aggregate(cbind(Sepal.Length, Petal.Length) ~ Species, data = iris, FUN = mean) avg_sepal_petal_by_species ## Species Sepal.Length Petal.Length ## 1 setosa 5.006 1.462 ## 2 versicolor 5.936 4.260 ## 3 virginica 6.588 5.552 ________________________________________________________________________________ 3.3 Data Reshaping Data reshaping is the process of transforming the layout or structure of a data set without changing the actual data. You typically reshape data to suit different analyses, visualizations, or reporting formats. Common operations for reshaping include pivoting data between wide and long formats. Wide format: Each subject(row) has its own columns for measurements at different time points or categories. Long format: The data has one measurement per row, making it easier to analyze in some cases, especially with repeated measures. In R, the most common function for reshaping data include; pivot_longer() and pivot_wider() from the tidyr package. melt() and dcast() from the reshape2 package. Try it! Let’s have some fun by working on the mtcars data set where we will demonstrate reshaping between wide and long formats Step 1: Inspect the Data The mtcars data set is already in a wide format where each row represents a car, and columns represent different variables for instance mpg, cyl, hp. data(mtcars) # Load the data set # First few records of the data set head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Step2: Converting from Wide to Long Format We will use the pivot_longer() function from the tidyr package to convert the data set from wide to long format. In this case, we will shape the mpg, hp and wt columns into a longer format making it easier to work with. library(tidyr) # Reshape the data from wide to long format mtcars_long &lt;- mtcars %&gt;% pivot_longer(cols=c(mpg, hp, wt), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) # View the respaed data head(mtcars_long) ## # A tibble: 6 × 10 ## cyl disp drat qsec vs am gear carb variable value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6 160 3.9 16.5 0 1 4 4 mpg 21 ## 2 6 160 3.9 16.5 0 1 4 4 hp 110 ## 3 6 160 3.9 16.5 0 1 4 4 wt 2.62 ## 4 6 160 3.9 17.0 0 1 4 4 mpg 21 ## 5 6 160 3.9 17.0 0 1 4 4 hp 110 ## 6 6 160 3.9 17.0 0 1 4 4 wt 2.88 If we break down the code; pivot_longer() function moves the selected columns (mpg, hp, wt) into a new “long” format, with eah row representing a unique combination of car characteristics(variable) and their corresponding value. names_to = \"variable\": The variable names (e.g., mpg, hp, wt) are moved to a column named “variable”. values_to = \"value\": The data for each variable is placed in a column named \"value\". Also, data in long format can be converted to a wide format. The pivot_wider function from dplyr gets the work done. Try it! Lets put the pivot_wider function into practice. We will convert the ntcars_long data set that we just recently generated to a wider format. # Reshape from long to wide format mtcars_wide &lt;- mtcars_long %&gt;% pivot_wider(names_from = &quot;variable&quot;, values_from = &quot;value&quot;) # View the reshaped data head(mtcars_wide) ## # A tibble: 6 × 11 ## cyl disp drat qsec vs am gear carb mpg hp wt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 160 3.9 16.5 0 1 4 4 21 110 2.62 ## 2 6 160 3.9 17.0 0 1 4 4 21 110 2.88 ## 3 4 108 3.85 18.6 1 1 4 1 22.8 93 2.32 ## 4 6 258 3.08 19.4 1 0 3 1 21.4 110 3.22 ## 5 8 360 3.15 17.0 0 0 3 2 18.7 175 3.44 ## 6 6 225 2.76 20.2 1 0 3 1 18.1 105 3.46 If we break down the code; pivot_wider() converts the long format back into the wide format, with separate columns for each variable (mpg, hp, wt). names_from = \"variable\": Moves the unique values from the \"variable” column into their own columns (e.g., mpg, hp, wt). values_from = \"value\": Populates the new columns with values from the “value” column. Practical Exercise Use the pivot_longer() function to convert the iris dataset (which contains measurements for different flower features) into a long format. Focus on converting the numeric columns like Sepal.Length and Sepal.Width. Then, use pivot_wider() to convert it back to a wide format. Solution Convert to long format library(tidyr) # Load the data data(iris) # Load the iris dataset and reshape it iris_long &lt;- iris %&gt;% pivot_longer(cols = starts_with(&quot;Sepal&quot;), names_to = &quot;feature&quot;, values_to = &quot;measurement&quot;) # View the reshaped data head(iris_long) ## # A tibble: 6 × 5 ## Petal.Length Petal.Width Species feature measurement ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.4 0.2 setosa Sepal.Length 5.1 ## 2 1.4 0.2 setosa Sepal.Width 3.5 ## 3 1.4 0.2 setosa Sepal.Length 4.9 ## 4 1.4 0.2 setosa Sepal.Width 3 ## 5 1.3 0.2 setosa Sepal.Length 4.7 ## 6 1.3 0.2 setosa Sepal.Width 3.2 Back to wide # Now reshape it back to wide format iris_wide &lt;- iris_long %&gt;% pivot_wider(names_from = &quot;feature&quot;, values_from = &quot;measurement&quot;) # View the reshaped data head(iris_wide) ## # A tibble: 6 × 5 ## Petal.Length Petal.Width Species Sepal.Length Sepal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 1.4 0.2 setosa &lt;dbl [8]&gt; &lt;dbl [8]&gt; ## 2 1.3 0.2 setosa &lt;dbl [4]&gt; &lt;dbl [4]&gt; ## 3 1.5 0.2 setosa &lt;dbl [7]&gt; &lt;dbl [7]&gt; ## 4 1.7 0.4 setosa &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 5 1.4 0.3 setosa &lt;dbl [3]&gt; &lt;dbl [3]&gt; ## 6 1.5 0.1 setosa &lt;dbl [2]&gt; &lt;dbl [2]&gt; 3.4 Hands-on Exercise In this practical exercise, you will be required to download and import the Furniture sales data set from here and perform the following; Calculate the average profit margin for each material using the tapply function. Find the difference between the cost and the price of each item on the mapply function. Calculate the maximum price, cost and sales columns using the sapply and lapply function. Note the difference between the two functions. Use aggregation to find the maximum sales for each product category. Subset the brand, sales columns and name it brand_df. Convert the data to a longer format and store in a variable pbrand_df_long. Solution Import the data set # Load the data set df &lt;- read.csv(&quot;data/Furniture.csv&quot;) head(df) ## price cost sales profit_margin inventory discount_percentage delivery_days ## 1 218.5431 181.61093 40 16.89924 105 27.796433 9 ## 2 477.8214 385.03383 7 19.41889 192 26.943715 6 ## 3 379.3973 276.73677 32 27.05884 59 21.948130 2 ## 4 319.3963 281.84133 48 11.75811 45 11.009944 2 ## 5 120.2084 69.74368 19 41.98102 35 3.183763 9 ## 6 120.1975 65.35208 6 45.62943 185 20.659352 8 ## category material color location season store_type brand revenue ## 1 Bed Plastic Red Rural Spring Online BrandA 3949.165 ## 2 Chair Glass Blue Rural Summer Online BrandD -3521.002 ## 3 Table Metal Black Suburban Fall Online BrandD 14285.560 ## 4 Table Glass Green Rural Summer Retail BrandD 12261.074 ## 5 Chair Glass Brown Rural Fall Online BrandD -4588.256 ## 6 Table Plastic Brown Urban Fall Retail BrandB 9136.302 Calculate the average profit margin for each material using the tapply function. average_profit_margin &lt;- tapply(df$profit_margin, df$material, mean) average_profit_margin ## Fabric Glass Metal Plastic Wood ## 29.23317 30.31741 30.67792 30.09190 30.57714 Find the difference between the cost and the price of each item on the mapply function. # Create a function that finds difference between two numbers diff_a_b &lt;- function(a, b){ d = a - b return(d) } # Difference between cost and price cost_price_difference &lt;- mapply(diff_a_b, df$price, df$cost) head(cost_price_difference) ## [1] 36.93212 92.78761 102.66051 37.55498 50.46471 54.84545 Calculate the maximum price, cost and sales columns using the sapply and lapply function. Note the difference between the two functions. # Subset the data set library(dplyr) subset_df &lt;- df %&gt;% dplyr::select(price, cost, sales) # Using sapply function sapply(subset_df, max) ## price cost sales ## 499.8730 447.0229 49.0000 # Using the lapply function lapply(subset_df, max) ## $price ## [1] 499.873 ## ## $cost ## [1] 447.0229 ## ## $sales ## [1] 49 Use aggregation to find the maximum sales for each product category. library(plyr) # Perform aggregation aggregated_sales &lt;- aggregate(sales ~ category, data = df, FUN = max) aggregated_sales ## category sales ## 1 Bed 49 ## 2 Chair 49 ## 3 Desk 49 ## 4 Sofa 49 ## 5 Table 49 Subset the brand, sales columns and name it brand_df. Convert the data to a longer format and store in a variable pbrand_df_long. library(tidyr) library(dplyr) # Subset the data set brand_df &lt;- df %&gt;% dplyr::select(brand, sales) head(brand_df) ## brand sales ## 1 BrandA 40 ## 2 BrandD 7 ## 3 BrandD 32 ## 4 BrandD 48 ## 5 BrandD 19 ## 6 BrandB 6 # To a longer format brand_df_long &lt;- brand_df %&gt;% pivot_longer( cols = &quot;brand&quot;) head(brand_df_long) ## # A tibble: 6 × 3 ## sales name value ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 40 brand BrandA ## 2 7 brand BrandD ## 3 32 brand BrandD ## 4 48 brand BrandD ## 5 19 brand BrandD ## 6 6 brand BrandB ________________________________________________________________________________ "],["general-statistics.html", "Chapter 4 General Statistics 4.1 Tabulating Factors and Creating Contingency Tables 4.2 Calculating Quantiles 4.3 4.4 z-Scores 4.4 Inferential Statistics 4.5 Testing the Mean of a Sample (t-Test) and its Confidence Interval 4.6 Testing a Sample Proportion and its Confidence Interval 4.7 Performing Pairwise Comparisons Between Group Means 4.8 Hands-on Exercise", " Chapter 4 General Statistics 4.1 Tabulating Factors and Creating Contingency Tables This section explores how to handle categorical data using factors and contingency tables in R. We will learn how to: 4.1.1 Understanding Factors in R Factors store categorical variables efficiently and allow statistical functions to recognize levels. 4.1.1.1 Example: Creating a Factor Variable # Creating a categorical variable survey_response &lt;- c(&quot;Agree&quot;, &quot;Disagree&quot;, &quot;Neutral&quot;, &quot;Agree&quot;, &quot;Agree&quot;, &quot;Disagree&quot;) # Convert to factor survey_factor &lt;- factor(survey_response) # Print the factor print(survey_factor) ## [1] Agree Disagree Neutral Agree Agree Disagree ## Levels: Agree Disagree Neutral 4.1.1.2 Example: Reordering Factor Levels survey_factor_ordered &lt;- factor(survey_response, levels = c(&quot;Disagree&quot;, &quot;Neutral&quot;, &quot;Agree&quot;), ordered = TRUE) print(survey_factor_ordered) ## [1] Agree Disagree Neutral Agree Agree Disagree ## Levels: Disagree &lt; Neutral &lt; Agree 4.1.2 Creating Frequency Tables A frequency table counts the number of occurrences of each category. 4.1.2.1 Example: Using table() to Create a Frequency Table table(survey_factor) ## survey_factor ## Agree Disagree Neutral ## 3 2 1 4.1.2.2 Example: Getting Proportions with prop.table() prop.table(table(survey_factor)) ## survey_factor ## Agree Disagree Neutral ## 0.5000000 0.3333333 0.1666667 4.1.3 Creating Contingency Tables A contingency table (cross-tabulation) is used to summarize two categorical variables. 4.1.3.1 Example: 2-Way Contingency Table # Sample data gender &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;) # Creating a contingency table contingency_table &lt;- table(gender, survey_factor) # Display the table print(contingency_table) ## survey_factor ## gender Agree Disagree Neutral ## Female 0 2 1 ## Male 3 0 0 4.1.4 4.2.4 Adding Margins to Contingency Tables We can add row and column totals using addmargins(). 4.1.4.1 Example: Adding Margins addmargins(contingency_table) ## survey_factor ## gender Agree Disagree Neutral Sum ## Female 0 2 1 3 ## Male 3 0 0 3 ## Sum 3 2 1 6 4.1.5 4.2.5 Computing Row and Column Proportions 4.1.5.1 Example: Row Proportions prop.table(contingency_table, margin = 1) ## survey_factor ## gender Agree Disagree Neutral ## Female 0.0000000 0.6666667 0.3333333 ## Male 1.0000000 0.0000000 0.0000000 4.1.5.2 Example: Column Proportions prop.table(contingency_table, margin = 2) ## survey_factor ## gender Agree Disagree Neutral ## Female 0 1 1 ## Male 1 0 0 4.1.6 Visualizing Contingency Tables 4.1.6.1 Example: Bar Plot barplot(contingency_table, beside = TRUE, legend = TRUE, col = c(&quot;blue&quot;, &quot;red&quot;)) 4.1.6.2 Example: Mosaic Plot mosaicplot(contingency_table, main = &quot;Survey Responses by Gender&quot;, col = c(&quot;skyblue&quot;, &quot;pink&quot;)) 4.1.7 Practical Exercises 4.1.7.1 Exercise 1: Working with Factors Create a factor variable from the following data: education &lt;- c(&quot;High School&quot;, &quot;College&quot;, &quot;College&quot;, &quot;PhD&quot;, &quot;Masters&quot;, &quot;High School&quot;) Convert it into an ordered factor with levels: \"High School\" &lt; \"College\" &lt; \"Masters\" &lt; \"PhD\" Print the ordered factor. Solution: education_factor &lt;- factor(education, levels = c(&quot;High School&quot;, &quot;College&quot;, &quot;Masters&quot;, &quot;PhD&quot;), ordered = TRUE) print(education_factor) ## [1] High School College College PhD Masters High School ## Levels: High School &lt; College &lt; Masters &lt; PhD 4.1.7.2 Exercise 2: Creating a Contingency Table Create two categorical variables: department &lt;- c(&quot;Sales&quot;, &quot;HR&quot;, &quot;IT&quot;, &quot;Sales&quot;, &quot;HR&quot;, &quot;IT&quot;, &quot;Sales&quot;, &quot;IT&quot;) status &lt;- c(&quot;Full-Time&quot;, &quot;Part-Time&quot;, &quot;Full-Time&quot;, &quot;Part-Time&quot;, &quot;Full-Time&quot;, &quot;Full-Time&quot;, &quot;Part-Time&quot;, &quot;Full-Time&quot;) Generate a contingency table. Compute row and column proportions. Add margins to the table. Solution: # Creating a contingency table dept_table &lt;- table(department, status) # Display the table print(dept_table) ## status ## department Full-Time Part-Time ## HR 1 1 ## IT 3 0 ## Sales 1 2 # Row proportions prop.table(dept_table, margin = 1) ## status ## department Full-Time Part-Time ## HR 0.5000000 0.5000000 ## IT 1.0000000 0.0000000 ## Sales 0.3333333 0.6666667 # Column proportions prop.table(dept_table, margin = 2) ## status ## department Full-Time Part-Time ## HR 0.2000000 0.3333333 ## IT 0.6000000 0.0000000 ## Sales 0.2000000 0.6666667 # Adding margins addmargins(dept_table) ## status ## department Full-Time Part-Time Sum ## HR 1 1 2 ## IT 3 0 3 ## Sales 1 2 3 ## Sum 5 3 8 4.1.7.3 Exercise 3: Titanic Dataset Analysis Use the built-in Titanic dataset to: Create a contingency table of passenger class (Pclass) and survival (Survived). Compute row and column proportions. Create a bar plot. Solution: # Load required packages library(dplyr) library(tidyr) # Load Titanic dataset data(Titanic) # Convert to a proper dataframe and expand the frequency count Titanic_df &lt;- as.data.frame(Titanic) %&gt;% uncount(Freq) # Expands rows based on the frequency column # Check the structure of the dataframe str(Titanic_df) ## &#39;data.frame&#39;: 2201 obs. of 4 variables: ## $ Class : Factor w/ 4 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ Sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Age : Factor w/ 2 levels &quot;Child&quot;,&quot;Adult&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Survived: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... # Create a contingency table using the correct column names titanic_table &lt;- table(Titanic_df$Class, Titanic_df$Survived) # Print the table print(titanic_table) ## ## No Yes ## 1st 122 203 ## 2nd 167 118 ## 3rd 528 178 ## Crew 673 212 4.2 Calculating Quantiles Quantiles are statistical measures that divide a dataset into equal parts. They help summarize distributions, identify outliers, and assess skewness. 4.2.1 Understanding Quantiles Quantiles divide data into equal-sized groups: Median (50th percentile): The middle value of a dataset. Quartiles (25th, 50th, 75th percentiles): Divide data into four equal parts. Deciles (10th, 20th, …, 90th percentiles): Divide data into ten equal parts. Percentiles (1st, 2nd, …, 99th percentiles): Divide data into 100 equal parts. 4.2.2 Computing Quantiles in R 4.2.2.1 Example: Finding Quartiles # Create a dataset data &lt;- c(3, 7, 8, 5, 12, 14, 21, 13, 18) ### Compute quartiles quantile(data) ## 0% 25% 50% 75% 100% ## 3 7 12 14 21 25% (Q1): First quartile 50% (Q2): Median 75% (Q3): Third quartile 100%: Maximum value 4.2.3 Computing Specific Quantiles 4.2.3.1 Example: Finding the 10th and 90th Percentiles quantile(data, probs = c(0.10, 0.90)) ## 10% 90% ## 4.6 18.6 4.2.4 Using summary() for Quick Insights summary(data) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 7.00 12.00 11.22 14.00 21.00 The summary() function provides: Min: Minimum value 1st Qu. (Q1, 25%): First quartile Median (Q2, 50%): Second quartile Mean: Average value 3rd Qu. (Q3, 75%): Third quartile Max: Maximum value 4.2.5 Visualizing Quantiles 4.2.5.1 Example: Boxplot to Show Quartiles boxplot(data, main = &quot;Boxplot of Data&quot;, col = &quot;lightblue&quot;) In a boxplot: The median (Q2) as a thick line in the box The interquartile range (IQR, Q1 to Q3) as the box Outliers as individual points 4.2.6 Finding Interquartile Range (IQR) The IQR measures the spread of the middle 50% of the data: IQR(data) ## [1] 7 OR manually: iqr_value &lt;- quantile(data, 0.75) - quantile(data, 0.25) print(iqr_value) ## 75% ## 7 4.2.7 Finding Outliers Using IQR Outliers are values outside Q1 - 1.5*IQR and Q3 + 1.5*IQR. 4.2.7.1 Example: Detecting Outliers # Compute quartiles q1 &lt;- quantile(data, 0.25) q3 &lt;- quantile(data, 0.75) iqr_value &lt;- IQR(data) # Define outlier thresholds lower_bound &lt;- q1 - 1.5 * iqr_value upper_bound &lt;- q3 + 1.5 * iqr_value # Find outliers outliers &lt;- data[data &lt; lower_bound | data &gt; upper_bound] print(outliers) ## numeric(0) 4.2.8 4.3.8 Hands-on Exercises 4.2.8.1 Exercise 1: Computing Quartiles Create a dataset: scores &lt;- c(55, 78, 85, 90, 92, 60, 73, 81, 95, 88) Compute Q1, Q2 (median), and Q3. Calculate the IQR. Plot a boxplot. Solution: quantile(scores) ## 0% 25% 50% 75% 100% ## 55.00 74.25 83.00 89.50 95.00 IQR(scores) ## [1] 15.25 boxplot(scores, main = &quot;Exam Scores&quot;, col = &quot;lightblue&quot;) 4.2.8.2 Exercise 2: Computing Custom Quantiles Use the dataset: heights &lt;- c(150, 160, 165, 170, 175, 180, 185, 190, 195, 200) Find the 5th, 25th, 50th, 75th, and 95th percentiles. Solution: quantile(heights, probs = c(0.05, 0.25, 0.50, 0.75, 0.95)) ## 5% 25% 50% 75% 95% ## 154.50 166.25 177.50 188.75 197.75 4.2.8.3 Exercise 3: Finding Outliers Use the dataset: salaries &lt;- c(40000, 42000, 45000, 47000, 50000, 52000, 55000, 58000, 60000, 100000) Compute Q1, Q3, and IQR. Identify outliers. Solution: q1 &lt;- quantile(salaries, 0.25) q3 &lt;- quantile(salaries, 0.75) iqr_value &lt;- IQR(salaries) lower_bound &lt;- q1 - 1.5 * iqr_value upper_bound &lt;- q3 + 1.5 * iqr_value outliers &lt;- salaries[salaries &lt; lower_bound | salaries &gt; upper_bound] print(outliers) ## [1] 1e+05 4.3 4.4 z-Scores The z-score (also called the standard score) tells us how many standard deviations a data point is from the mean. It is a useful measure for comparing values across different distributions and detecting outliers. 4.3.1 4.4.1 Understanding z-Scores The z-score formula is: \\[ z = \\frac{x - \\mu}{\\sigma} \\] Where: \\(x\\) = data point \\(\\mu\\) = mean of the dataset \\(\\sigma\\) = standard deviation of the dataset Interpretation: \\(z = 0\\) → The value is equal to the mean. \\(z &gt; 0\\) → The value is above the mean. \\(z &lt; 0\\) → The value is below the mean. \\(|z| &gt; 2\\) → The value is unusual. \\(|z| &gt; 3\\) → The value is potentially an outlier. 4.3.2 4.4.2 Computing z-Scores in R 4.3.2.1 Example: Computing z-Scores Manually # Sample data data &lt;- c(50, 55, 60, 65, 70, 75, 80, 85, 90, 95) # Compute mean and standard deviation mean_value &lt;- mean(data) sd_value &lt;- sd(data) # Compute z-scores z_scores &lt;- (data - mean_value) / sd_value print(z_scores) ## [1] -1.4863011 -1.1560120 -0.8257228 -0.4954337 -0.1651446 0.1651446 0.4954337 ## [8] 0.8257228 1.1560120 1.4863011 4.3.3 Computing z-Scores Using scale() The scale() function standardizes a dataset (converts it into z-scores): # Compute z-scores using scale() z_scaled &lt;- scale(data) print(z_scaled) ## [,1] ## [1,] -1.4863011 ## [2,] -1.1560120 ## [3,] -0.8257228 ## [4,] -0.4954337 ## [5,] -0.1651446 ## [6,] 0.1651446 ## [7,] 0.4954337 ## [8,] 0.8257228 ## [9,] 1.1560120 ## [10,] 1.4863011 ## attr(,&quot;scaled:center&quot;) ## [1] 72.5 ## attr(,&quot;scaled:scale&quot;) ## [1] 15.13825 The scale() function automatically centers and scales the data. 4.3.4 4.4.4 Interpreting z-Scores Let’s compute the z-score of 80 from our dataset: z_80 &lt;- (80 - mean_value) / sd_value print(z_80) ## [1] 0.4954337 If \\(z = 0.5\\), this means 80 is 0.5 standard deviations above the mean. 4.3.5 4.4.5 Using z-Scores to Detect Outliers Outliers are values that have \\(|z| &gt; 3\\). 4.3.5.1 Example: Finding Outliers # Identify values with |z| &gt; 3 outliers &lt;- data[abs(z_scores) &gt; 3] print(outliers) ## numeric(0) 4.3.6 4.4.6 Visualizing z-Scores 4.3.6.1 Example: Histogram of z-Scores hist(z_scores, main = &quot;Histogram of z-Scores&quot;, col = &quot;skyblue&quot;, xlab = &quot;z-Scores&quot;) abline(v = c(-3, 3), col = &quot;red&quot;, lwd = 2) # Marking outlier thresholds 4.3.6.2 Example: Standard Normal Curve with z-Scores x &lt;- seq(-4, 4, length=100) y &lt;- dnorm(x) plot(x, y, type=&quot;l&quot;, lwd=2, col=&quot;blue&quot;, main=&quot;Standard Normal Distribution&quot;) abline(v = c(-3, -2, -1, 0, 1, 2, 3), col=&quot;red&quot;, lty=2) # Mark z-scores 4.3.7 Hands-on Exercises 4.3.7.1 Exercise 1: Compute z-Scores Use the dataset: heights &lt;- c(150, 160, 165, 170, 175, 180, 185, 190, 195, 200) Compute the mean and standard deviation. Calculate the z-scores. Find any outliers (\\(|z| &gt; 3\\)). Solution: mean_height &lt;- mean(heights) sd_height &lt;- sd(heights) z_scores &lt;- (heights - mean_height) / sd_height outliers &lt;- heights[abs(z_scores) &gt; 3] print(z_scores) ## [1] -1.6853070 -1.0611192 -0.7490253 -0.4369314 -0.1248376 0.1872563 0.4993502 ## [8] 0.8114441 1.1235380 1.4356319 print(outliers) ## numeric(0) 4.3.7.2 Exercise 2: Standardize Data Using scale() Use the dataset: weights &lt;- c(55, 60, 65, 70, 75, 80, 85, 90, 95, 100) Standardize the data using scale(). Plot a histogram of the z-scores. Solution: z_weights &lt;- scale(weights) hist(z_weights, main = &quot;Histogram of Standardized Weights&quot;, col = &quot;lightgreen&quot;) 4.3.7.3 Exercise 3: Identifying Outliers Use the dataset: salaries &lt;- c(40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000, 200000) Compute z-scores. Identify outliers. Solution: mean_salary &lt;- mean(salaries) sd_salary &lt;- sd(salaries) z_salaries &lt;- (salaries - mean_salary) / sd_salary outliers &lt;- salaries[abs(z_salaries) &gt; 3] print(outliers) ## numeric(0) 4.4 Inferential Statistics Inferential statistics allows us to make conclusions about a population based on a sample. 4.4.1 Basic Concepts 4.4.1.1 Population vs. Sample Population: The entire group we want to study. Sample: A subset of the population used for analysis. 4.4.1.2 Parameter vs. Statistic Parameter: A value that describes the population. Statistic: A value computed from a sample. 4.4.1.3 Common Inferential Techniques Confidence Intervals – Estimating population values. Hypothesis Testing – Testing claims about populations. 4.4.2 Confidence Intervals A confidence interval (CI) gives a range where we expect a population parameter to lie. 4.4.2.1 Example: Confidence Interval for a Mean # Sample data data &lt;- c(50, 55, 60, 65, 70, 75, 80, 85, 90, 95) # Mean and standard deviation mean_data &lt;- mean(data) sd_data &lt;- sd(data) n &lt;- length(data) # Compute confidence interval (95% confidence) error_margin &lt;- qt(0.975, df=n-1) * (sd_data / sqrt(n)) lower_bound &lt;- mean_data - error_margin upper_bound &lt;- mean_data + error_margin # Print confidence interval c(lower_bound, upper_bound) ## [1] 61.67075 83.32925 We are 95% confident that the population mean lies within this range. 4.4.3 Hypothesis Testing Hypothesis testing helps us determine whether a claim about a population is supported by sample data. 4.4.4 Steps in Hypothesis Testing State the null (\\(H_0\\)) and alternative (\\(H_A\\)) hypotheses. Select a significance level (\\(\\alpha\\)). Compute the test statistic. Compare the test statistic to a critical value or p-value. Make a conclusion. 4.4.5 One-Sample t-Test Tests if the sample mean is different from a known population mean. 4.4.5.1 Example: Testing If a Sample Mean Differs from 70 # Sample data sample_data &lt;- c(65, 68, 72, 75, 70, 66, 71, 69, 74, 67) # One-sample t-test (H0: Mean = 70) t.test(sample_data, mu = 70) ## ## One Sample t-test ## ## data: sample_data ## t = -0.28446, df = 9, p-value = 0.7825 ## alternative hypothesis: true mean is not equal to 70 ## 95 percent confidence interval: ## 67.31429 72.08571 ## sample estimates: ## mean of x ## 69.7 If p-value &lt; 0.05, reject \\(H_0\\). If p-value &gt; 0.05, fail to reject \\(H_0\\). 4.4.6 Comparing Two Sample Means A two-sample t-test compares the means of two independent groups. 4.4.6.1 Example: Comparing Male vs. Female Heights # Sample data male_heights &lt;- c(170, 175, 180, 185, 190, 195) female_heights &lt;- c(160, 165, 168, 170, 175, 178) # Two-sample t-test t.test(male_heights, female_heights) ## ## Welch Two Sample t-test ## ## data: male_heights and female_heights ## t = 2.8225, df = 8.9621, p-value = 0.02005 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.607164 23.726169 ## sample estimates: ## mean of x mean of y ## 182.5000 169.3333 If p-value &lt; 0.05, the means are significantly different. If p-value &gt; 0.05, the means are not significantly different. 4.4.7 Testing Proportions A proportion test is used for categorical data. 4.4.7.1 Example: Testing if 60% of People Prefer Brand A # Sample data: 55 people prefer Brand A out of 100 prop.test(55, 100, p = 0.60) ## ## 1-sample proportions test with continuity correction ## ## data: 55 out of 100, null probability 0.6 ## X-squared = 0.84375, df = 1, p-value = 0.3583 ## alternative hypothesis: true p is not equal to 0.6 ## 95 percent confidence interval: ## 0.4475426 0.6485719 ## sample estimates: ## p ## 0.55 If p-value &lt; 0.05, reject \\(H_0\\). If p-value &gt; 0.05, fail to reject \\(H_0\\). 4.4.8 4.5.7 Practical Exercises 4.4.8.1 Exercise 1: Confidence Interval for Population Mean Use the dataset: weights &lt;- c(55, 60, 65, 70, 75, 80, 85, 90, 95, 100) Compute a 95% confidence interval for the population mean. Solution: mean_weights &lt;- mean(weights) sd_weights &lt;- sd(weights) n_weights &lt;- length(weights) # Compute CI error_margin &lt;- qt(0.975, df=n_weights-1) * (sd_weights / sqrt(n_weights)) c(mean_weights - error_margin, mean_weights + error_margin) ## [1] 66.67075 88.32925 4.4.8.2 Exercise 2: Hypothesis Test for a Mean Use the dataset: test_scores &lt;- c(78, 82, 85, 90, 88, 79, 84, 87, 92, 81) Test whether the mean test score is greater than 80. Solution: t.test(test_scores, mu = 80, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: test_scores ## t = 3.1139, df = 9, p-value = 0.00622 ## alternative hypothesis: true mean is greater than 80 ## 95 percent confidence interval: ## 81.89206 Inf ## sample estimates: ## mean of x ## 84.6 4.4.8.3 Exercise 3: Comparing Two Sample Means Create two samples: group_A &lt;- c(15, 18, 20, 22, 25, 27, 30) group_B &lt;- c(17, 19, 21, 24, 26, 28, 32) Perform a two-sample t-test. Solution: t.test(group_A, group_B) ## ## Welch Two Sample t-test ## ## data: group_A and group_B ## t = -0.50767, df = 12, p-value = 0.6209 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.559670 4.702527 ## sample estimates: ## mean of x mean of y ## 22.42857 23.85714 4.4.8.4 Exercise 4: Testing a Sample Proportion Suppose 45 out of 100 people prefer Product X. Test if the true proportion is different from 50%. Solution: prop.test(45, 100, p = 0.50) ## ## 1-sample proportions test with continuity correction ## ## data: 45 out of 100, null probability 0.5 ## X-squared = 0.81, df = 1, p-value = 0.3681 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.3514281 0.5524574 ## sample estimates: ## p ## 0.45 4.5 Testing the Mean of a Sample (t-Test) and its Confidence Interval A t-test is used to test whether the mean of a sample is significantly different from a hypothesized population mean. It helps answer questions like: “Is the average test score significantly different from 70?” “Does the sample data suggest a real effect, or is it due to random chance?” 4.5.1 Understanding the t-Test The one-sample t-test formula: \\[ t = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}} \\] Where: \\(\\bar{x}\\) = sample mean \\(\\mu\\) = population mean \\(s\\) = sample standard deviation \\(n\\) = sample size 4.5.1.1 Key Assumptions: The data is normally distributed (or \\(n &gt; 30\\)). The sample is randomly selected. The standard deviation is unknown (if known, use a z-test instead). 4.5.2 Computing Confidence Intervals for the Mean A confidence interval (CI) provides a range where the true population mean is expected to lie. 4.5.2.1 Example: 95% Confidence Interval for a Sample Mean # Sample data data &lt;- c(55, 60, 65, 70, 75, 80, 85, 90, 95, 100) # Compute mean, standard deviation, and sample size mean_data &lt;- mean(data) sd_data &lt;- sd(data) n &lt;- length(data) # Compute confidence interval (95% confidence level) error_margin &lt;- qt(0.975, df=n-1) * (sd_data / sqrt(n)) lower_bound &lt;- mean_data - error_margin upper_bound &lt;- mean_data + error_margin # Print confidence interval c(lower_bound, upper_bound) ## [1] 66.67075 88.32925 We are 95% confident that the population mean lies within this range. 4.5.3 Performing a One-Sample t-Test A one-sample t-test checks whether the sample mean is significantly different from a given value. 4.5.3.1 Example: Testing if the Mean is Different from 70 # Sample data sample_data &lt;- c(65, 68, 72, 75, 70, 66, 71, 69, 74, 67) # Perform one-sample t-test t.test(sample_data, mu = 70) ## ## One Sample t-test ## ## data: sample_data ## t = -0.28446, df = 9, p-value = 0.7825 ## alternative hypothesis: true mean is not equal to 70 ## 95 percent confidence interval: ## 67.31429 72.08571 ## sample estimates: ## mean of x ## 69.7 If p-value &lt; 0.05, reject \\(H_0\\) → The mean is significantly different from 70. If p-value &gt; 0.05, fail to reject \\(H_0\\) → No significant difference. 4.5.4 One-Sided vs. Two-Sided Tests By default, t.test() performs a two-sided test (\\(H_A: \\mu \\neq 70\\)). If we want to test whether the mean is greater than or less than a value: 4.5.4.1 Example: Testing if Mean is Greater Than 70 t.test(sample_data, mu = 70, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: sample_data ## t = -0.28446, df = 9, p-value = 0.6088 ## alternative hypothesis: true mean is greater than 70 ## 95 percent confidence interval: ## 67.76676 Inf ## sample estimates: ## mean of x ## 69.7 4.5.4.2 Example: Testing if Mean is Less Than 70 t.test(sample_data, mu = 70, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: sample_data ## t = -0.28446, df = 9, p-value = 0.3912 ## alternative hypothesis: true mean is less than 70 ## 95 percent confidence interval: ## -Inf 71.63324 ## sample estimates: ## mean of x ## 69.7 4.5.5 Comparing Two Sample Means (Independent t-Test) A two-sample t-test checks if two groups have significantly different means. 4.5.5.1 Example: Comparing Male vs. Female Heights # Sample data male_heights &lt;- c(170, 175, 180, 185, 190, 195) female_heights &lt;- c(160, 165, 168, 170, 175, 178) # Perform independent t-test t.test(male_heights, female_heights) ## ## Welch Two Sample t-test ## ## data: male_heights and female_heights ## t = 2.8225, df = 8.9621, p-value = 0.02005 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.607164 23.726169 ## sample estimates: ## mean of x mean of y ## 182.5000 169.3333 4.5.6 Paired t-Test (Dependent Samples) A paired t-test compares before and after measurements. 4.5.6.1 Example: Testing Before vs. After Training Scores # Scores before and after training before &lt;- c(60, 65, 70, 75, 80, 85, 90) after &lt;- c(65, 68, 75, 78, 85, 88, 92) # Perform paired t-test t.test(before, after, paired = TRUE) ## ## Paired t-test ## ## data: before and after ## t = -7.8393, df = 6, p-value = 0.0002277 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -4.873641 -2.554930 ## sample estimates: ## mean difference ## -3.714286 4.5.7 Practical Exercises 4.5.7.1 Exercise 1: Compute a Confidence Interval Use the dataset: scores &lt;- c(78, 82, 85, 90, 88, 79, 84, 87, 92, 81) Compute a 95% confidence interval for the mean. Solution: mean_scores &lt;- mean(scores) sd_scores &lt;- sd(scores) n_scores &lt;- length(scores) # Compute CI error_margin &lt;- qt(0.975, df=n_scores-1) * (sd_scores / sqrt(n_scores)) c(mean_scores - error_margin, mean_scores + error_margin) ## [1] 81.25826 87.94174 4.5.7.2 Exercise 2: One-Sample t-Test Use the dataset: weights &lt;- c(55, 60, 65, 70, 75, 80, 85, 90, 95, 100) Test whether the mean is different from 72. Solution: t.test(weights, mu = 72) ## ## One Sample t-test ## ## data: weights ## t = 1.1489, df = 9, p-value = 0.2802 ## alternative hypothesis: true mean is not equal to 72 ## 95 percent confidence interval: ## 66.67075 88.32925 ## sample estimates: ## mean of x ## 77.5 4.5.7.3 Exercise 3: Comparing Two Groups Use the dataset: group_A &lt;- c(15, 18, 20, 22, 25, 27, 30) group_B &lt;- c(17, 19, 21, 24, 26, 28, 32) Perform an independent two-sample t-test. Solution: t.test(group_A, group_B) ## ## Welch Two Sample t-test ## ## data: group_A and group_B ## t = -0.50767, df = 12, p-value = 0.6209 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.559670 4.702527 ## sample estimates: ## mean of x mean of y ## 22.42857 23.85714 4.5.7.4 Exercise 4: Paired t-Test A study measures reaction time before and after caffeine consumption: before_caffeine &lt;- c(300, 320, 310, 305, 315, 290, 295) after_caffeine &lt;- c(280, 300, 290, 285, 295, 275, 280) Perform a paired t-test to determine if caffeine affects reaction time. Solution: t.test(before_caffeine, after_caffeine, paired = TRUE) ## ## Paired t-test ## ## data: before_caffeine and after_caffeine ## t = 20.14, df = 6, p-value = 9.733e-07 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 16.31504 20.82782 ## sample estimates: ## mean difference ## 18.57143 4.6 Testing a Sample Proportion and its Confidence Interval A proportion test is used when we want to make inferences about categorical data. This test helps us: Estimate the proportion of a population with a certain characteristic. Determine whether a sample proportion differs significantly from a hypothesized value. 4.6.1 Understanding Proportion Testing A sample proportion is calculated as: \\[ \\hat{p} = \\frac{x}{n} \\] Where: \\(x\\) = Number of successes (e.g., people who answered “Yes”) \\(n\\) = Total number of observations The confidence interval (CI) for a proportion is given by: \\[ \\hat{p} \\pm Z \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\] Where: \\(Z\\) = Critical value for the confidence level (e.g., 1.96 for 95%) \\(\\hat{p}\\) = Sample proportion \\(n\\) = Sample size 4.6.2 Computing Confidence Intervals for Proportions We can calculate confidence intervals for proportions using prop.test(). 4.6.2.1 Example: 95% Confidence Interval for Proportion Suppose 60 out of 100 people prefer Brand A. # Number of successes (people preferring Brand A) x &lt;- 60 # Total sample size n &lt;- 100 # Compute confidence interval prop.test(x, n, conf.level = 0.95, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 4, df = 1, p-value = 0.0455 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.5020026 0.6905987 ## sample estimates: ## p ## 0.6 We are 95% confident that the true proportion of people who prefer Brand A falls within the computed confidence interval. 4.6.3 Performing a One-Sample Proportion Test We test whether a sample proportion is significantly different from a hypothesized proportion \\(p_0\\). 4.6.3.1 Example: Testing if 60% Prefer Brand A We test: \\[ H_0: p = 0.60 \\] \\[ H_A: p \\neq 0.60 \\] prop.test(x, n, p = 0.60, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.6 ## X-squared = 0, df = 1, p-value = 1 ## alternative hypothesis: true p is not equal to 0.6 ## 95 percent confidence interval: ## 0.5020026 0.6905987 ## sample estimates: ## p ## 0.6 If p-value &lt; 0.05, reject \\(H_0\\) → The sample proportion is significantly different from 60%. If p-value &gt; 0.05, fail to reject \\(H_0\\) → No significant difference. 4.6.4 One-Sided Proportion Tests If we want to test if the proportion is greater than or less than a given value: 4.6.4.1 Example: Testing if Proportion is Greater Than 50% prop.test(x, n, p = 0.50, alternative = &quot;greater&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.5 ## X-squared = 4, df = 1, p-value = 0.02275 ## alternative hypothesis: true p is greater than 0.5 ## 95 percent confidence interval: ## 0.5178095 1.0000000 ## sample estimates: ## p ## 0.6 4.6.4.2 Example: Testing if Proportion is Less Than 70% prop.test(x, n, p = 0.70, alternative = &quot;less&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: x out of n, null probability 0.7 ## X-squared = 4.7619, df = 1, p-value = 0.01455 ## alternative hypothesis: true p is less than 0.7 ## 95 percent confidence interval: ## 0.0000000 0.6769219 ## sample estimates: ## p ## 0.6 4.6.5 Comparing Two Sample Proportions We can compare two proportions to determine if they are significantly different. 4.6.5.1 Example: Comparing Success Rates of Two Groups Group 1: 30 successes out of 50 Group 2: 45 successes out of 80 prop.test(c(30, 45), c(50, 80), correct = FALSE) ## ## 2-sample test for equality of proportions without continuity correction ## ## data: c(30, 45) out of c(50, 80) ## X-squared = 0.17727, df = 1, p-value = 0.6737 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1364425 0.2114425 ## sample estimates: ## prop 1 prop 2 ## 0.6000 0.5625 If p-value &lt; 0.05, the two proportions are significantly different. If p-value &gt; 0.05, no significant difference. 4.6.6 Visualizing Proportions 4.6.6.1 Example: Bar Plot of Proportions successes &lt;- c(30, 45) total &lt;- c(50, 80) proportions &lt;- successes / total barplot(proportions, names.arg = c(&quot;Group 1&quot;, &quot;Group 2&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), main = &quot;Comparison of Two Proportions&quot;, ylim = c(0, 1), ylab = &quot;Proportion&quot;) 4.6.7 Practical Exercises 4.6.7.1 Exercise 1: Compute a Confidence Interval A survey shows that 150 out of 500 people support a new policy. Compute a 95% confidence interval for the proportion. Solution: prop.test(150, 500, conf.level = 0.95, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 150 out of 500, null probability 0.5 ## X-squared = 80, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.2614819 0.3415678 ## sample estimates: ## p ## 0.3 4.6.7.2 Exercise 2: One-Sample Proportion Test A sample of 200 students finds that 140 prefer online learning. Test if the proportion is different from 65%. Solution: prop.test(140, 200, p = 0.65, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 140 out of 200, null probability 0.65 ## X-squared = 2.1978, df = 1, p-value = 0.1382 ## alternative hypothesis: true p is not equal to 0.65 ## 95 percent confidence interval: ## 0.6332093 0.7592526 ## sample estimates: ## p ## 0.7 4.6.7.3 Exercise 3: One-Sided Proportion Test In a company, 45 out of 100 employees prefer remote work. Test if the proportion is greater than 40%. Solution: prop.test(45, 100, p = 0.40, alternative = &quot;greater&quot;, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 45 out of 100, null probability 0.4 ## X-squared = 1.0417, df = 1, p-value = 0.1537 ## alternative hypothesis: true p is greater than 0.4 ## 95 percent confidence interval: ## 0.370561 1.000000 ## sample estimates: ## p ## 0.45 4.6.7.4 Exercise 4: Comparing Two Proportions Two groups were surveyed: Group A: 85 out of 150 prefer a new product. Group B: 75 out of 130 prefer the new product. Test whether the proportions are significantly different. Solution: prop.test(c(85, 75), c(150, 130), correct = FALSE) ## ## 2-sample test for equality of proportions without continuity correction ## ## data: c(85, 75) out of c(150, 130) ## X-squared = 0.029915, df = 1, p-value = 0.8627 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.1264510 0.1059382 ## sample estimates: ## prop 1 prop 2 ## 0.5666667 0.5769231 ## Comparing the Means of Two Samples Comparing the means of two independent samples is essential in determining if there is a significant difference between two groups. 4.6.8 Understanding Two-Sample t-Test The two-sample t-test checks whether the means of two independent groups are significantly different. Hypotheses: Null Hypothesis (\\(H_0\\)): The two group means are equal. Alternative Hypothesis (\\(H_A\\)): The two group means are different. \\[ t = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\] Where: \\(\\bar{x_1}, \\bar{x_2}\\) = Sample means \\(s_1, s_2\\) = Standard deviations \\(n_1, n_2\\) = Sample sizes 4.6.9 Independent (Unpaired) t-Test This test is used when the two samples are independent. 4.6.9.1 Example: Comparing Heights of Males and Females # Sample data male_heights &lt;- c(170, 175, 180, 185, 190, 195) female_heights &lt;- c(160, 165, 168, 170, 175, 178) # Perform independent t-test t.test(male_heights, female_heights) ## ## Welch Two Sample t-test ## ## data: male_heights and female_heights ## t = 2.8225, df = 8.9621, p-value = 0.02005 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.607164 23.726169 ## sample estimates: ## mean of x mean of y ## 182.5000 169.3333 If p-value &lt; 0.05, reject \\(H_0\\) → The two means are significantly different. If p-value &gt; 0.05, fail to reject \\(H_0\\) → No significant difference. 4.6.10 Checking Assumptions Before running a t-test, we must check: Normality (Use Shapiro-Wilk test) Equal Variances (Use F-test) 4.6.10.1 Example: Checking Normality shapiro.test(male_heights) ## ## Shapiro-Wilk normality test ## ## data: male_heights ## W = 0.98189, p-value = 0.9606 shapiro.test(female_heights) ## ## Shapiro-Wilk normality test ## ## data: female_heights ## W = 0.9841, p-value = 0.97 4.6.10.2 Example: Checking Equal Variances var.test(male_heights, female_heights) ## ## F test to compare two variances ## ## data: male_heights and female_heights ## F = 2.0317, num df = 5, denom df = 5, p-value = 0.4551 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2843024 14.5195451 ## sample estimates: ## ratio of variances ## 2.031734 If p-value &lt; 0.05, variances are not equal → Use var.equal = FALSE in t.test(). If p-value &gt; 0.05, variances are equal → Use var.equal = TRUE. 4.6.11 Performing t-Test with Unequal Variances 4.6.11.1 Example: When Variances are Unequal t.test(male_heights, female_heights, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: male_heights and female_heights ## t = 2.8225, df = 8.9621, p-value = 0.02005 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.607164 23.726169 ## sample estimates: ## mean of x mean of y ## 182.5000 169.3333 4.6.11.2 Example: When Variances are Equal t.test(male_heights, female_heights, var.equal = TRUE) ## ## Two Sample t-test ## ## data: male_heights and female_heights ## t = 2.8225, df = 10, p-value = 0.01808 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 2.772665 23.560668 ## sample estimates: ## mean of x mean of y ## 182.5000 169.3333 4.6.12 Paired t-Test (Dependent Samples) A paired t-test is used when the same subjects are measured twice (e.g., before and after treatment). 4.6.12.1 Example: Testing Before vs. After Training Scores # Scores before and after training before &lt;- c(60, 65, 70, 75, 80, 85, 90) after &lt;- c(65, 68, 75, 78, 85, 88, 92) # Perform paired t-test t.test(before, after, paired = TRUE) ## ## Paired t-test ## ## data: before and after ## t = -7.8393, df = 6, p-value = 0.0002277 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -4.873641 -2.554930 ## sample estimates: ## mean difference ## -3.714286 4.6.13 Visualizing Group Differences 4.6.13.1 Example: Boxplot Comparing Two Groups # Combine data into a dataframe data &lt;- data.frame( Height = c(male_heights, female_heights), Gender = rep(c(&quot;Male&quot;, &quot;Female&quot;), each = 6) ) # Plot boxplot boxplot(Height ~ Gender, data = data, col = c(&quot;blue&quot;, &quot;red&quot;), main = &quot;Height Comparison&quot;) 4.6.14 Practical Exercises 4.6.14.1 Exercise 1: Independent t-Test Two groups take an exam: group_A &lt;- c(78, 80, 85, 88, 90, 92, 95) group_B &lt;- c(75, 78, 82, 85, 87, 89, 91) Test if their mean scores are significantly different. Solution: t.test(group_A, group_B) ## ## Welch Two Sample t-test ## ## data: group_A and group_B ## t = 0.92929, df = 11.951, p-value = 0.3711 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.037004 10.037004 ## sample estimates: ## mean of x mean of y ## 86.85714 83.85714 4.6.14.2 Exercise 2: Checking Assumptions Use the dataset: data_1 &lt;- c(10, 12, 14, 16, 18, 20) data_2 &lt;- c(8, 9, 10, 12, 14, 15) Check for normality and equal variances. Solution: shapiro.test(data_1) ## ## Shapiro-Wilk normality test ## ## data: data_1 ## W = 0.98189, p-value = 0.9606 shapiro.test(data_2) ## ## Shapiro-Wilk normality test ## ## data: data_2 ## W = 0.94009, p-value = 0.6599 var.test(data_1, data_2) ## ## F test to compare two variances ## ## data: data_1 and data_2 ## F = 1.7797, num df = 5, denom df = 5, p-value = 0.5423 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2490297 12.7181372 ## sample estimates: ## ratio of variances ## 1.779661 4.6.14.3 Exercise 3: Paired t-Test A fitness test is conducted before and after training: before_fitness &lt;- c(50, 55, 60, 62, 65, 67, 70) after_fitness &lt;- c(55, 58, 63, 65, 68, 70, 73) Test if there is a significant improvement after training. Solution: t.test(before_fitness, after_fitness, paired = TRUE) ## ## Paired t-test ## ## data: before_fitness and after_fitness ## t = -11.5, df = 6, p-value = 2.597e-05 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -3.984832 -2.586597 ## sample estimates: ## mean difference ## -3.285714 4.6.15 Exercise 4: Visualizing Group Differences Create two datasets: treatment &lt;- c(100, 110, 120, 130, 140) control &lt;- c(95, 105, 115, 125, 135) Plot a boxplot to compare the groups. Solution: # Combine data into dataframe data &lt;- data.frame( Score = c(treatment, control), Group = rep(c(&quot;Treatment&quot;, &quot;Control&quot;), each = 5) ) # Plot boxplot boxplot(Score ~ Group, data = data, col = c(&quot;blue&quot;, &quot;red&quot;), main = &quot;Treatment vs. Control&quot;) 4.7 Performing Pairwise Comparisons Between Group Means When comparing more than two groups, pairwise comparisons allow us to identify which groups differ significantly. Common methods include: t-Tests with adjustments for multiple comparisons Tukey’s Honest Significant Difference (HSD) test Bonferroni correction Dunnett’s test (comparing to a control group) 4.7.1 Understanding Pairwise Comparisons When comparing multiple groups, running multiple t-tests increases the risk of Type I errors (false positives). To correct this, we apply multiple comparison adjustments like: Bonferroni correction (divides alpha by the number of comparisons) Holm correction (stepwise adjustment) Tukey’s HSD (for ANOVA post-hoc comparisons) 4.7.2 Performing Pairwise t-Tests The pairwise.t.test() function performs multiple t-tests while adjusting for multiple comparisons. 4.7.2.1 Example: Comparing Exam Scores Across Three Groups # Sample data group &lt;- rep(c(&quot;Group A&quot;, &quot;Group B&quot;, &quot;Group C&quot;), each = 5) scores &lt;- c(85, 88, 90, 92, 94, 78, 80, 83, 85, 87, 70, 72, 75, 77, 79) ### Perform pairwise t-tests with Bonferroni correction pairwise.t.test(scores, group, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: scores and group ## ## Group A Group B ## Group B 0.024 - ## Group C 6.8e-05 0.013 ## ## P value adjustment method: bonferroni The output provides p-values for each pairwise comparison. If p-value &lt; 0.05, the groups significantly differ. 4.7.3 Tukey’s HSD Test Tukey’s Honest Significant Difference (HSD) test is used after ANOVA to compare all groups. 4.7.3.1 Example: Tukey’s HSD Test # Create dataset data &lt;- data.frame( Group = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 5)), Score = c(85, 88, 90, 92, 94, 78, 80, 83, 85, 87, 70, 72, 75, 77, 79) ) # Perform ANOVA anova_model &lt;- aov(Score ~ Group, data = data) # Tukey&#39;s HSD Test TukeyHSD(anova_model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Score ~ Group, data = data) ## ## $Group ## diff lwr upr p adj ## B-A -7.2 -13.26805 -1.131954 0.0206029 ## C-A -15.2 -21.26805 -9.131954 0.0000620 ## C-B -8.0 -14.06805 -1.931954 0.0109527 The test provides confidence intervals for differences between groups. If p-value &lt; 0.05, the groups significantly differ. 4.7.4 Bonferroni and Holm Corrections The Bonferroni correction divides alpha (0.05) by the number of comparisons. The Holm correction adjusts p-values stepwise, maintaining more power. 4.7.4.1 Example: Comparing Methods pairwise.t.test(scores, group, p.adjust.method = &quot;holm&quot;) # Holm ## ## Pairwise comparisons using t tests with pooled SD ## ## data: scores and group ## ## Group A Group B ## Group B 0.0085 - ## Group C 6.8e-05 0.0085 ## ## P value adjustment method: holm pairwise.t.test(scores, group, p.adjust.method = &quot;bonferroni&quot;) # Bonferroni ## ## Pairwise comparisons using t tests with pooled SD ## ## data: scores and group ## ## Group A Group B ## Group B 0.024 - ## Group C 6.8e-05 0.013 ## ## P value adjustment method: bonferroni pairwise.t.test(scores, group, p.adjust.method = &quot;BH&quot;) # Benjamini-Hochberg ## ## Pairwise comparisons using t tests with pooled SD ## ## data: scores and group ## ## Group A Group B ## Group B 0.0081 - ## Group C 6.8e-05 0.0064 ## ## P value adjustment method: BH Bonferroni is more conservative. Holm maintains statistical power. BH (Benjamini-Hochberg) controls the false discovery rate. 4.7.5 Dunnett’s Test (Comparing to a Control Group) Dunnett’s test compares all groups against a control group. 4.7.5.1 Example: Comparing Treatment Groups to a Control # Create dataset data &lt;- data.frame( Treatment = factor(rep(c(&quot;Control&quot;, &quot;Drug A&quot;, &quot;Drug B&quot;), each = 5)), Response = c(50, 55, 53, 52, 54, 60, 62, 65, 67, 64, 70, 72, 75, 78, 77) ) # Perform ANOVA anova_model &lt;- aov(Response ~ Treatment, data = data) # Perform Dunnett’s test library(multcomp) summary(glht(anova_model, linfct = mcp(Treatment = &quot;Dunnett&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Dunnett Contrasts ## ## ## Fit: aov(formula = Response ~ Treatment, data = data) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## Drug A - Control == 0 10.800 1.724 6.263 7.98e-05 *** ## Drug B - Control == 0 21.600 1.724 12.527 5.79e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Compares Drug A and Drug B to the Control. p-values tell if treatments differ from the control. 4.7.6 Practical Exercises 4.7.6.1 Exercise 1: Perform Pairwise Comparisons Create three groups of exam scores: students &lt;- rep(c(&quot;Class A&quot;, &quot;Class B&quot;, &quot;Class C&quot;), each = 6) scores &lt;- c(78, 80, 82, 85, 88, 90, 70, 73, 75, 77, 78, 80, 60, 62, 65, 68, 70, 72) Perform pairwise t-tests with Holm correction. Solution: pairwise.t.test(scores, students, p.adjust.method = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: scores and students ## ## Class A Class B ## Class B 0.0046 - ## Class C 1.2e-05 0.0041 ## ## P value adjustment method: holm 4.7.6.2 Exercise 2: Tukey’s HSD Test Create three treatment groups: group &lt;- rep(c(&quot;Control&quot;, &quot;Treatment A&quot;, &quot;Treatment B&quot;), each = 5) values &lt;- c(10, 12, 15, 13, 14, 18, 20, 22, 21, 23, 25, 27, 30, 29, 31) Perform ANOVA and Tukey’s HSD test. Solution: anova_model &lt;- aov(values ~ group) TukeyHSD(anova_model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = values ~ group) ## ## $group ## diff lwr upr p adj ## Treatment A-Control 8.0 4.460679 11.53932 0.0001624 ## Treatment B-Control 15.6 12.060679 19.13932 0.0000002 ## Treatment B-Treatment A 7.6 4.060679 11.13932 0.0002584 4.7.6.3 Exercise 3: Comparing to a Control Group A clinical trial tests three conditions: condition &lt;- rep(c(&quot;Control&quot;, &quot;Low Dose&quot;, &quot;High Dose&quot;), each = 6) blood_pressure &lt;- c(130, 128, 132, 129, 131, 130, 125, 123, 120, 124, 126, 122, 115, 113, 118, 116, 117, 114) Perform Dunnett’s test to compare treatments against the control. Solution: # Ensure condition is a factor condition &lt;- factor(condition) anova_model &lt;- aov(blood_pressure ~ condition) library(multcomp) summary(glht(anova_model, linfct = mcp(condition = &quot;Dunnett&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Dunnett Contrasts ## ## ## Fit: aov(formula = blood_pressure ~ condition) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## High Dose - Control == 0 -14.500 1.063 -13.643 1.44e-09 *** ## Low Dose - Control == 0 -6.667 1.063 -6.273 2.90e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) 4.8 Hands-on Exercise 4.8.1 Exercise 1: Descriptive Statistics Create a dataset of monthly sales revenue: revenue &lt;- c(12000, 13500, 14200, 16000, 17000, 12500, 14000, 15000, 15500, 16500) Compute: Mean, median, standard deviation Minimum and maximum values Interquartile range (IQR) Solution # Compute summary statistics summary(revenue) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12000 13625 14600 14620 15875 17000 # Standard deviation sd(revenue) ## [1] 1673.187 # Interquartile range IQR(revenue) ## [1] 2250 4.8.2 Exercise 2: Confidence Interval for Mean Use the dataset: test_scores &lt;- c(65, 70, 75, 80, 85, 90, 95, 100, 105, 110) Compute a 95% confidence interval for the mean. Solution mean_test_scores &lt;- mean(test_scores) sd_test_scores &lt;- sd(test_scores) n &lt;- length(test_scores) # Compute confidence interval error_margin &lt;- qt(0.975, df=n-1) * (sd_test_scores / sqrt(n)) c(mean_test_scores - error_margin, mean_test_scores + error_margin) ## [1] 76.67075 98.32925 4.8.3 Exercise 3: One-Sample t-Test** Use the dataset: weights &lt;- c(55, 60, 65, 70, 75, 80, 85, 90, 95, 100) Test if the mean weight is significantly different from 72. Solution t.test(weights, mu = 72) ## ## One Sample t-test ## ## data: weights ## t = 1.1489, df = 9, p-value = 0.2802 ## alternative hypothesis: true mean is not equal to 72 ## 95 percent confidence interval: ## 66.67075 88.32925 ## sample estimates: ## mean of x ## 77.5 4.8.4 Exercise 4: Proportion Test A survey finds that 65 out of 120 respondents prefer a new product. Test if the proportion is different from 50%. Solution prop.test(65, 120, p = 0.50, correct = FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 65 out of 120, null probability 0.5 ## X-squared = 0.83333, df = 1, p-value = 0.3613 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4526097 0.6281387 ## sample estimates: ## p ## 0.5416667 4.8.5 Exercise 5: Comparing Two Sample Means Two classes take a math test: class_A &lt;- c(78, 80, 85, 88, 90, 92, 95) class_B &lt;- c(75, 78, 82, 85, 87, 89, 91) Test if their mean scores are significantly different. Solution t.test(class_A, class_B) ## ## Welch Two Sample t-test ## ## data: class_A and class_B ## t = 0.92929, df = 11.951, p-value = 0.3711 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.037004 10.037004 ## sample estimates: ## mean of x mean of y ## 86.85714 83.85714 4.8.6 Exercise 6: Data Visualization Use the dataset: categories &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;) Create a bar chart. Solution category_table &lt;- table(categories) barplot(category_table, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), main = &quot;Category Distribution&quot;, xlab = &quot;Category&quot;, ylab = &quot;Count&quot;) 4.8.7 Exercise 7: Scatterplot with Regression Line Create two variables: experience &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) salary &lt;- c(40000, 42000, 45000, 47000, 50000, 52000, 55000, 58000, 60000, 63000) Create a scatterplot with a regression line. Solution plot(experience, salary, col = &quot;blue&quot;, pch = 19, main = &quot;Experience vs Salary&quot;, xlab = &quot;Years of Experience&quot;, ylab = &quot;Salary ($)&quot;) abline(lm(salary ~ experience), col = &quot;red&quot;, lwd = 2) 4.8.8 Exercise 8: Pairwise Comparisons Create a dataset with three groups: group &lt;- rep(c(&quot;Group A&quot;, &quot;Group B&quot;, &quot;Group C&quot;), each = 5) values &lt;- c(85, 88, 90, 92, 94, 78, 80, 83, 85, 87, 70, 72, 75, 77, 79) Perform pairwise t-tests. Solution pairwise.t.test(values, group, p.adjust.method = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: values and group ## ## Group A Group B ## Group B 0.0085 - ## Group C 6.8e-05 0.0085 ## ## P value adjustment method: holm 4.8.9 Exercise 9: Tukey’s HSD Test Use the dataset: treatment &lt;- rep(c(&quot;Control&quot;, &quot;Treatment A&quot;, &quot;Treatment B&quot;), each = 5) response &lt;- c(50, 55, 53, 52, 54, 60, 62, 65, 67, 64, 70, 72, 75, 78, 77) Perform ANOVA and Tukey’s HSD test. Solution anova_model &lt;- aov(response ~ treatment) TukeyHSD(anova_model) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = response ~ treatment) ## ## $treatment ## diff lwr upr p adj ## Treatment A-Control 10.8 6.199708 15.40029 0.0001144 ## Treatment B-Control 21.6 16.999708 26.20029 0.0000001 ## Treatment B-Treatment A 10.8 6.199708 15.40029 0.0001144 4.8.10 Exercise 10: Comparing a Treatment to a Control A clinical trial tests three conditions: condition &lt;- rep(c(&quot;Control&quot;, &quot;Low Dose&quot;, &quot;High Dose&quot;), each = 6) blood_pressure &lt;- c(130, 128, 132, 129, 131, 130, 125, 123, 120, 124, 126, 122, 115, 113, 118, 116, 117, 114) Perform Dunnett’s test to compare treatments against the control. Solution # Ensure condition is a factor condition &lt;- factor(condition) anova_model &lt;- aov(blood_pressure ~ condition) library(multcomp) summary(glht(anova_model, linfct = mcp(condition = &quot;Dunnett&quot;))) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Dunnett Contrasts ## ## ## Fit: aov(formula = blood_pressure ~ condition) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## High Dose - Control == 0 -14.500 1.063 -13.643 1.44e-09 *** ## Low Dose - Control == 0 -6.667 1.063 -6.273 2.90e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) ________________________________________________________________________________ "],["simple-linear-regression.html", "Chapter 5 Simple Linear Regression 5.1 Basics of Wilkinson-Rogers Notation (y ~ x), Linear Regression 5.2 Scatterplots with Regression Lines, Reading lm() Output 5.3 Confidence Intervals for Regression Coefficients, Testing Coefficients 5.4 Identifying Points in a Plot 5.5 Hands-On Exercise", " Chapter 5 Simple Linear Regression Welcome to the world of Simple Linear Regression!  This statistical technique is super handy when you want to explore the relationship between two continuous variables. Essentially, it helps us predict the value of one variable based on the value of another. For example, imagine you want to predict a student’s exam score based on the number of hours they studied. Here, the hours studied are the independent variable (or predictor), and the exam score is the dependent variable (or response). What is Simple Linear Regression? Simple linear regression is essentially about finding the line that best describes the relationship between two variables(independent and dependent variables) in your data. The line is called the regression line, and it helps us make predictions. 5.1 Basics of Wilkinson-Rogers Notation (y ~ x), Linear Regression In simple linear regression, we fit a straight line (called the regression line) through the data points. This line is defined by the equation: \\[y = mx + b\\] Where: \\(y\\) is the predicted value (dependent variable). \\(m\\) is the slope of the line (how much \\(y\\) changes for a unit change in \\(x\\)). \\(x\\) is the independent variable. \\(b\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is 0). Try it! Lets say, we have a data set of age and height of youngsters as below here: age = c(5, 10, 15, 20, 25) height = c(110, 130, 150, 160, 170) We can use the age to predict the height where age is the independent variable and height is the dependent variable(depends on age). The equation is written like this in R; height ~ age in short independent_variable ~ dependent_variable. Now, lets work it out using the lm() function # Creating a sample data set age = c(5, 10, 15, 20, 25) height = c(110, 130, 150, 160, 170) # Fitting a linear regression data set model_1 &lt;- lm(height ~ age) The lm() function stands for ’linear model` and it finds the best line to describle the relationship between height and age. summary(model_1) ## ## Call: ## lm(formula = height ~ age) ## ## Residuals: ## 1 2 3 4 5 ## -4 1 6 1 -4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.0000 5.0662 19.54 0.000293 *** ## age 3.0000 0.3055 9.82 0.002245 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.83 on 3 degrees of freedom ## Multiple R-squared: 0.9698, Adjusted R-squared: 0.9598 ## F-statistic: 96.43 on 1 and 3 DF, p-value: 0.002245 Real world challenge Let’s use the built-in mtcars data set in R to demonstrate how to perform simple linear regression. Load the data set # Load the mtcars dataset data(mtcars) # View the first few rows of the dataset head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Fit the simple linear regression model that will predict mpg (miles per gallon) based on wt (the weight of the car). # Fit the linear regression model model_2 &lt;- lm(mpg ~ wt, data = mtcars) Get the model summary to get important information about the model we just fitted. # Get the summary of the model summary(model_2) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 5.2 Scatterplots with Regression Lines, Reading lm() Output Now that we have a model in place lets plot the data and regression line to understand the relationship. You remember we have just worked on a case of where we use the age of youngsters to find their height? That’s fine! Lets plot a scatter plot and regression line to visualize the relationship between age and height. # Scatter plot of age vs height plot(age, height, main = &quot;Height vs Age&quot;, xlab = &quot;Age&quot;, ylab = &quot;Height&quot;,pch=19, col = &quot;blue&quot; ) The scatter plot shows that the youngsters tend to be taller as they get older. Now lets add a regression line to the data to see how well it fits the data; # Scatter plot of age vs height plot(age, height, main = &quot;Height vs Age&quot;, xlab = &quot;Age&quot;, ylab = &quot;Height&quot;,pch=19, col = &quot;blue&quot; ) # Adding a regression line abline(model_1, col=&quot;red&quot;, lwd=2) The regression line helps us visually understand the trend. In our case here, the regression line closely follows the data points, therefore, our model is a good fit! Real World example We will still consider the model that we created above for the mtcars data set. Lets plot a scatter plot and fit a regression line based on the model that we have just created. # Plot the data points plot(mtcars$wt, mtcars$mpg, main = &quot;Simple Linear Regression&quot;, xlab = &quot;Weight of the Car (wt)&quot;, ylab = &quot;Miles Per Gallon (mpg)&quot;, pch = 19, col = &quot;blue&quot;) # Add the regression line abline(model_2, col = &quot;red&quot;) Predictions can be made based on the data. Lets predict the mpg for car that weighs 3.5 tons # Predict mpg for a car that weighs 3.5 tons new_data &lt;- data.frame(wt = 3.5) predicted_mpg &lt;- predict(model_2, new_data) print(paste(&quot;Predicted MPG for a car weighing 3.5 tons:&quot;, round(predicted_mpg, 2))) ## [1] &quot;Predicted MPG for a car weighing 3.5 tons: 18.58&quot; 5.3 Confidence Intervals for Regression Coefficients, Testing Coefficients Lets revisit the model that we fitted age and height of youths. summary(model_1) ## ## Call: ## lm(formula = height ~ age) ## ## Residuals: ## 1 2 3 4 5 ## -4 1 6 1 -4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.0000 5.0662 19.54 0.000293 *** ## age 3.0000 0.3055 9.82 0.002245 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.83 on 3 degrees of freedom ## Multiple R-squared: 0.9698, Adjusted R-squared: 0.9598 ## F-statistic: 96.43 on 1 and 3 DF, p-value: 0.002245 Here we will focus on the coefficients and the confidence intervals. The confidence interval for each regression coefficient helps us understand the range within which true value of the coeefficient is likely to fall. The confint() function in R is used to obtain this information. # Calculate the confidence interval confint(model_1) ## 2.5 % 97.5 % ## (Intercept) 82.877001 115.122999 ## age 2.027747 3.972253 As stated earlier the simple linear regression model equation is \\(y = mx + b\\); we will remodel this equation to fit the 2.5% and 97.5% confidence interval inform of \\(height(y) = Coefficient(m) * age(x) + Intercept(b)\\). Therefore: At 2.5% confidence interval; the equation is \\[height = 2.028 * age + 82.877\\] At 97.5% confidence interval; the equations is \\[height = 3.972 * age + 115.123\\] Using the quations above you can estimate the height of the youth based on age. Practical Exercise In this exercise, you will be required to use the built-in R mtcars data set. Use the weight(wt) to predict the fuel consumption(mpg). Find the equation with the confidence interval. Solution # Load the data data(mtcars) # Fit the linear regression model model_2 &lt;- lm(mpg ~ wt, data = mtcars) # Confidence intervals confint(model_2) ## 2.5 % 97.5 % ## (Intercept) 33.450500 41.119753 ## wt -6.486308 -4.202635 At 2.5% confidence interval, the equation to find mpg is \\[mpg = -6.486 * wt + 33.351\\] At 97.5% confidence interval, the equation to find mpg is \\[mpg = -4.203 * wt + 41.120\\] ________________________________________________________________________________ 5.4 Identifying Points in a Plot We made the scatter plot above static. Here, we will enable any researcher to identify specific points in the chart. I will now plot the chart using ggplot2 library. Lets do it library(ggplot2) # Create the data set height_data &lt;- data.frame( age = c(5, 10, 15, 20, 25), height = c(110, 130, 150, 160, 170) ) # Plot the and label the points ggplot(height_data, aes(x = age, y = height)) + geom_point(color = &quot;blue&quot;, size=3) + # for scatter plot geom_smooth(method = &quot;lm&quot;, color=&quot;red&quot;, se = FALSE) + # regression line geom_text( aes(label = paste(&quot;(&quot;, age, &quot;,&quot;, height, &quot;)&quot;, sep=&quot;&quot;)), vjust = -1, color = &quot;darkgreen&quot;) + # For interactiveness labs( title = &quot;Age vs Height with Identified points&quot;, x = &quot;Age&quot;, y = &quot;Height&quot; ) + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Try it! Lets repeat the same with the mtcars data set, we will plot a weight vs fuel consumption (mpg) with identified points in the plots library(ggplot2) # Load the data data(mtcars) # Plotting ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point(color = &quot;blue&quot;, size=3) + # for scatter plot geom_smooth(method = &quot;lm&quot;, color=&quot;red&quot;, se = FALSE) + # regression line geom_text( aes(label = paste(&quot;(&quot;, wt, &quot;,&quot;, mpg, &quot;)&quot;, sep=&quot;&quot;)), vjust = -1, color = &quot;darkgreen&quot;) + # For interactiveness labs( title = &quot;Wight vs mpg with Identified points&quot;, x = &quot;Weight&quot;, y = &quot;mpg&quot; ) + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 5.5 Hands-On Exercise In this exercise, you are required to download the boston housing data set from here and answer the following questions. Write the simple linear regression equation in form of \\(y=mx + b\\). Import the data and fit the linear regression model using the lm function to find the relationship between the average number of rooms(RM) and the housing price(MEDV). MEDV is the target variable while RM is the independent variable. Generate the summary of the model Create a scatter plot; x = RM and y = MEDV. Add a regression line to the scatter plot. Use the confint() function to find the coefficients at 2.5% and 97.5 confidence intervals. Solution Load the data and libraries library(ggplot2) # Import the data boston_housing_df &lt;- read.csv(&quot;data/boston_housing.csv&quot;) Write the simple linear regression equation. \\[MEDV = Coefficient * RM + Intercept\\] Import the data and fit the linear regression model using the lm function to find the relationship between the average number of rooms(RM) and the housing price(MEDV). MEDV is the target variable while RM is the independent variable. Generate the summary of the model. rm &lt;- boston_housing_df$RM medv &lt;- boston_housing_df$MEDV # Fit a linear regression model model_3 &lt;- lm(medv ~ rm) # Summary of the model summary(model_3) ## ## Call: ## lm(formula = medv ~ rm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.346 -2.547 0.090 2.986 39.433 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -34.671 2.650 -13.08 &lt;2e-16 *** ## rm 9.102 0.419 21.72 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.616 on 504 degrees of freedom ## Multiple R-squared: 0.4835, Adjusted R-squared: 0.4825 ## F-statistic: 471.8 on 1 and 504 DF, p-value: &lt; 2.2e-16 Create a scatter plot; x = RM and y = MEDV. # Scatter plot of number of rooms and price plot(rm, medv, main = &quot;Price vs Number of rooms&quot;, xlab = &quot;Number of rooms&quot;, ylab = &quot;Price&quot;,pch=19, col = &quot;blue&quot; ) Add a regression line to the scatter plot. # Scatter plot of number of rooms and price plot(rm, medv, main = &quot;Price vs Number of rooms&quot;, xlab = &quot;Number of rooms&quot;, ylab = &quot;Price&quot;,pch=19, col = &quot;blue&quot; ) # Add a regression line abline(model_3, col=&quot;red&quot;, lwd=2) Use the confint() function to find the coefficients at 2.5% and 97.5 confidence intervals at rewrite the equations with values at each confidence interval. confint(model_3) ## 2.5 % 97.5 % ## (Intercept) -39.876641 -29.464601 ## rm 8.278855 9.925363 At 2.5% confidence interval, the equation is \\[MEDV = 8.279 * RM - 39.877\\] At 97.5 confidence level, the equation is \\[MEDV = 9.925 * RM - 29.465\\] ________________________________________________________________________________ "],["reproducibility-and-report-with-r-markdown.html", "Chapter 6 Reproducibility and Report with R Markdown 6.1 Key Tools in R for Reproducibility and Reporting 6.2 Creating Reproducible Reports 6.3 Going Beyond: Shiny for Interactive Reporting", " Chapter 6 Reproducibility and Report with R Markdown Reproducibility is one of the core values in data science and R makes it both achievable and easy! Imagine trying to recreate someone’s analysis only to find that you get different results or that they left out crucial steps. Frustrating, right? Reproducibility is the answer—it means you can get the same results every time by following the same steps. Why Reproducibility Matters Trustworthiness: When your results can be replicated, others can trust your analysis. Error Detection: Re-running the same code helps catch mistakes early. Efficiency: With reproducible scripts, you save time if you need to redo parts of your analysis. 6.1 Key Tools in R for Reproducibility and Reporting Let’s dive into the tools that make reproducibility and reporting a breeze in R: R Markdown: This is the gold standard for reproducible reports in R. You can write code, comments, and format it all beautifully in one document. Think of it as combining your code with a notebook-style narrative. Interactive Demo: Create an R Markdown file in RStudio by clicking File &gt; New File &gt; R Markdown…. You can add headers, code chunks, and text. Run Your Code: Run each chunk individually, or click Knit to create a fully formatted report with all your code and outputs embedded. Setting a Seed for Consistency: R’s random number generator can be controlled with set.seed(). For instance; set.seed(42) sample(1:100, 5) ## [1] 49 65 25 74 18 This will always produce the same random sample, making your analysis consistent. Code Commenting and Documentation: Clear comments make your analysis easy to understand for others and for yourself. Use comments (#) in your code to describe steps, and include documentation for more complex functions. Below is an example of a comment. # This is a comment 6.2 Creating Reproducible Reports Let’s walk through a simple activity where we create a reproducible report: Set Up Your R Markdown File Open RStudio and create a new R Markdown file. Add a title, your name, and the date. Start with an introduction: Below is an example of a report to introduce R makrdown. Insert the relevant details and press Ok to create a markdown file. An introductory report explaining how markdown works will be automatically generated. For more information about R makrdown visit here Add Your Code and Analysis Insert code chunks for each analysis step. For example, try loading and summarizing the mtcars data set: # Load the data data(mtcars) # Summary of the data set summary(mtcars) ## mpg cyl disp hp drat ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 ## wt qsec vs am gear ## Min. :1.513 Min. :14.50 Min. :0.0000 Min. :0.0000 Min. :3.000 ## 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:3.000 ## Median :3.325 Median :17.71 Median :0.0000 Median :0.0000 Median :4.000 ## Mean :3.217 Mean :17.85 Mean :0.4375 Mean :0.4062 Mean :3.688 ## 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:4.000 ## Max. :5.424 Max. :22.90 Max. :1.0000 Max. :1.0000 Max. :5.000 ## carb ## Min. :1.000 ## 1st Qu.:2.000 ## Median :2.000 ## Mean :2.812 ## 3rd Qu.:4.000 ## Max. :8.000 Customize and Style Your Report Add section headers, bold text, and bullet points to organize your report. You can use ggplot2 to add visualizations for a polished look. library(ggplot2) ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + labs(title = &quot;Horsepower vs. Miles per Gallon&quot;) Knit the Report Click the Knit button to render your report into an HTML, PDF, or Word document. Notice how your code, output, and comments are all integrated. Here is how the report should look like when knitted. 6.3 Going Beyond: Shiny for Interactive Reporting For advanced projects, consider using Shiny to create interactive reports! Shiny apps can run right in your browser and allow users to interact with your data in real time. More details on RShiny will be discussed later on the next topic Reproducibility is a powerful skill—keep practicing, and you’ll quickly see how it enhances your data work! Hands-on Exercises Create an R Markdown file with: A title and introduction explaining your analysis. An example dataset analysis (try using iris or mtcars). A basic visualization. A conclusion summarizing your findings. Knit the report to html Solution Instructor to show the students the example report(r Markdown and hmtl files) at path reproducibility_projects/example/ directory Solution provided in reproducibility_projects/solution/solution.Rmd Here is how the documents should look like ________________________________________________________________________________ "],["r-shiny.html", "Chapter 7 R Shiny 7.1 Structure of a Shiny App 7.2 Hands-On Exercise", " Chapter 7 R Shiny Shiny is a fantastic R package that allows you to easily create interactive web applications (or “apps”) directly from R. In this lesson, we’ll dive right into how to start building Shiny apps. First things first, if you haven’t installed the Shiny package yet, simply open R, make sure you’re connected to the internet, and run the following command: install.packages(&quot;shiny&quot;) Shiny also integrates with another package called bslib, which helps in creating visually appealing user interfaces (UIs). To explore more about it, you can check out its documentation here. Here is an example of an R shiny app The Shiny package comes with several pre-built examples that showcase how Shiny works in action. Each example is a fully functional Shiny app. The Hello Shiny example, for instance, generates a histogram using R’s faithful data set. The histogram’s bin count can be adjusted by the user through a slider, and the app instantly updates based on their selection. This example is perfect for learning the basics of Shiny app structure and building your very first app. To try it out, just run the following commands in R: library(shiny) runExample(&quot;01_hello&quot;) 7.1 Structure of a Shiny App Shiny apps are typically organized in a single script called app.R, which resides in a designated folder (for example, newdir/). You can run the app by executing runApp(\"newdir\"). The app.R file consists of three main components: A User Interface Object A Server Function A Call to the shinyApp Function 7.1.1 User Interface(ui) The user interface (ui) object defines the layout and visual aspects of your app. Below is the ui object used in the Hello Shiny example: library(shiny) library(bslib) # Define UI for app that draws a histogram ---- ui &lt;- page_sidebar( # App title ---- title = &quot;Hello Shiny!&quot;, # Sidebar panel for inputs ---- sidebar = sidebar( # Input: Slider for the number of bins ---- sliderInput( inputId = &quot;bins&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 30 ) ), # Output: Histogram ---- plotOutput(outputId = &quot;distPlot&quot;) ) 7.1.2 Server Here is the server function for the Hello Shiny Example: # Define server logic required to draw a histogram ---- server &lt;- function(input, output) { # Histogram of the Old Faithful Geyser Data ---- # with requested number of bins # This expression that generates a histogram is wrapped in a call # to renderPlot to indicate that: # # 1. It is &quot;reactive&quot; and should automatically # re-execute when inputs (input$bins) change # 2. Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful$waiting bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = &quot;#007bc2&quot;, border = &quot;white&quot;, xlab = &quot;Waiting time to next eruption (in mins)&quot;, main = &quot;Histogram of waiting times&quot;) }) } At its core, the server function in the Hello Shiny example is quite straightforward. It performs some calculations and generates a histogram based on the specified number of bins. However, you’ll notice that most of the script is wrapped in a call to renderPlot. The comment above the function provides some explanation, but don’t worry if it seems unclear; we will explore this concept in greater detail later. Before you start experimenting with the Hello Shiny app and reviewing its source code, remember that your app.R file should begin with loading the Shiny package and conclude with a call to shinyApp: library(shiny) # See above for the definitions of ui and server ui &lt;- ... server &lt;- ... shinyApp(ui = ui, server = server) While the Hello Shiny app is running, your R session will be occupied and won’t accept other commands. R is actively monitoring the app and processing its reactions. To regain access to your R session, press the escape key or click the stop icon located in the upper right corner of the RStudio console panel. How that you have known how to create an RShiny app, lets create a simple temperature converter that will convert temperature from Celcius to Farenheit. library(shiny) # Define UI for the Temperature Converter app ui &lt;- fluidPage( titlePanel(&quot;Temperature Converter&quot;), sidebarLayout( sidebarPanel( numericInput(&quot;temp_input&quot;, &quot;Temperature:&quot;, value = 0), selectInput(&quot;temp_scale&quot;, &quot;Select scale:&quot;, choices = c(&quot;Celsius to Fahrenheit&quot;, &quot;Fahrenheit to Celsius&quot;)), actionButton(&quot;convert&quot;, &quot;Convert&quot;) ), mainPanel( textOutput(&quot;result&quot;) ) ) ) # Define server logic for the Temperature Converter server &lt;- function(input, output) { observeEvent(input$convert, { if (input$temp_scale == &quot;Celsius to Fahrenheit&quot;) { result &lt;- (input$temp_input * 9/5) + 32 output$result &lt;- renderText({ paste(input$temp_input, &quot;°C =&quot;, round(result, 2), &quot;°F&quot;) }) } else { result &lt;- (input$temp_input - 32) * 5/9 output$result &lt;- renderText({ paste(input$temp_input, &quot;°F =&quot;, round(result, 2), &quot;°C&quot;) }) } }) } # Run the app shinyApp(ui = ui, server = server) Create a folder called \"shiny_app_demo\" and create an R script file called app.R where you will write the above code. After importing shiny and bslib libraries in your current environment run the app by executing the code below library(shiny) runApp(&quot;shiny_app_demo&quot;) This is how the app is expected to show up. Note: Everything will stop until you close the app Try it! We will create a shiny app based on the mtcars built-in data set. This app lets you visualize the mpg agains any other variable in the data set. Follow the steps below; Create a directory called \"shiny_apps\". Inside the shiny_app directory, create another directory called app_01. In the app_01 directory, create an R script file called app.R. Add the lines below to import the necessary libraries library(shiny) Create a UI component by adding the lines below # UI Component ui &lt;- fluidPage( titlePanel(&quot;Scatter Plot Visualizer&quot;), sidebarLayout( sidebarPanel( selectInput(&quot;variable&quot;, &quot;Choose Variable for x-axis:&quot;, choices = names(mtcars), selected = &quot;hp&quot;) ), mainPanel( plotOutput(&quot;scatterPlot&quot;) ) ) ) Now add the lines below to create a server component # Server Component server &lt;- function(input, output) { output$scatterPlot &lt;- renderPlot({ plot(mtcars[[input$variable]], mtcars$mpg, xlab = input$variable, ylab = &quot;mpg&quot;, main = paste(&quot;mpg vs&quot;, input$variable)) }) } Finally, run the app by adding the line below towards the end of the script # Run the App shinyApp(ui = ui, server = server) Run the below script in a different environment and watch the app in action! library(shiny) runApp(&quot;shiny_apps/app_01&quot;) Try it again! How about we make a bar chart using the built-in iris data set? Yes, we will have the bar chart that allows us to filter different iris species and visualize each characteristics separately. Lets get into action! Follow the steps below. Create a director app_02 and add a new R script file called app.R. Add the code below to app.R to import the necessary library library(shiny) library(ggplot2) Add the ui components with the sidePanel(where the control takes place) and the mainPanel (Where the chart shows up). Add the lines of code below. # UI Component ui &lt;- fluidPage( titlePanel(&quot;Bar Chart Explorer&quot;), sidebarLayout( sidebarPanel( checkboxGroupInput(&quot;species&quot;, &quot;Select Species:&quot;, choices = unique(iris$Species), selected = unique(iris$Species)) ), mainPanel( plotOutput(&quot;barChart&quot;) ) ) ) Add the server component where the calculations will take place. Use the renderPlot to show the app that you need to plot. # Server Component server &lt;- function(input, output) { output$barChart &lt;- renderPlot({ filtered_data &lt;- iris[iris$Species %in% input$species, ] ggplot(filtered_data, aes(Species)) + geom_bar(fill = &quot;steelblue&quot;) + labs(title = &quot;Species Count&quot;, x = &quot;Species&quot;, y = &quot;Count&quot;) }) } Add the lines below to complete the script # Run the App shinyApp(ui = ui, server = server) Finally, run the code below in a different environment to get the app running. library(shiny) runApp(&quot;shiny_apps/app_02&quot;) 7.2 Hands-On Exercise In this exercise, you will be required to download the Boston housing data set from here. Follow the instructions below to create a shiny app. Load the data set Your shiny app should have a simple structure with; A sidebar panel for inputs A main panel to display visualizations Implement the following functionalities in your app; A scatter plot that will allow the user to select an independent variable from the data set(e.g crime rate, number of rooms) to plot against the median house value (MEDV). Provide an option for the user to view a histogram of the median house value to understand the distribution of the housing prices. Allow the user to select the categorical variable like CHAS to compare distributions of MEDV using a boxplot. Make sure you use labels and titles for each title to make visualizations easy to understand. Also, comment your code to explain what each part does. Here are the app layout requirements; Side bar panel A dropdown menu for selecting an independent variable (e.g., CRIM, RM, TAX) for the scatter plot. A checkbox to view the histogram of MEDV. A dropdown menu for selecting the categorical variable (CHAS) for the box plot. Main Panel A scatter plot that shows the relationship between the selected variable and MEDV. A histogram of MEDV when selected. A box plot of MEDV grouped by CHAS. Remember to do your work in a folder assignment that will be inside the shiny_apps directory. Create an app.R script. Solution Check the app.R file in shiny_apps/assignment/ directory. This is how the app is created and run; Create a directory assignment in the shiny_apps. Within the assignment directory create an R script app.R. Add the code below to install and import the necessary libraries. # Install the library install.packages(&quot;shiny&quot;) # Load necessary libraries library(shiny) library(ggplot2) # for plotting Thereafter, add the lines to below to the app.R file to create a UI component. # Define UI for the app ui &lt;- fluidPage( titlePanel(&quot;Boston Housing Data Explorer&quot;), sidebarLayout( sidebarPanel( selectInput(&quot;xvar&quot;, &quot;Choose a variable for Scatter Plot:&quot;, choices = names(boston_df)[-14], # Excluding &#39;medv&#39; as it&#39;s the dependent variable selected = &quot;RM&quot;), # Default selection selectInput(&quot;box_var&quot;, &quot;Choose a variable for Box Plot:&quot;, choices = names(boston_df), selected = &quot;MEDV&quot;), checkboxInput(&quot;show_hist&quot;, &quot;Show Histogram of MEDV&quot;, FALSE) ), mainPanel( plotOutput(&quot;scatterPlot&quot;), plotOutput(&quot;boxPlot&quot;), plotOutput(&quot;histPlot&quot;) ) ) ) Now that the UI is ready, add the lines below to create the server logic for the app. # Define server logic for the app server &lt;- function(input, output) { # Scatter Plot: Selected variable vs Median House Value (medv) output$scatterPlot &lt;- renderPlot({ ggplot(boston_df, aes_string(x = input$xvar, y = &quot;MEDV&quot;)) + geom_point(color = &quot;blue&quot;, alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;, se = FALSE) + labs(title = paste(&quot;Scatter Plot of&quot;, input$xvar, &quot;vs Median House Value&quot;), x = input$xvar, y = &quot;Median House Value (MEDV)&quot;) + theme_minimal() }) # Box Plot of MEDV by Categorical Variable output$boxPlot &lt;- renderPlot({ ggplot(boston_df, aes_string(y = input$box_var)) + geom_boxplot(fill = &quot;lightgreen&quot;, color = &quot;darkgreen&quot;) + labs(title = paste(&quot;Box Plot of Median House Value by&quot;, input$box_var), x = input$box_var) + theme_minimal() }) # Histogram of MEDV output$histPlot &lt;- renderPlot({ if (input$show_hist) { ggplot(boston_df, aes(x = MEDV)) + geom_histogram(binwidth = 5, fill = &quot;skyblue&quot;, color = &quot;black&quot;, alpha = 0.7) + labs(title = &quot;Histogram of Median House Value (MEDV)&quot;, x = &quot;Median House Value&quot;, y = &quot;Frequency&quot;) + theme_minimal() } }) } The UI and server is ready. Finally, we need to add the lines below to enable the app to run. # Run the app shinyApp(ui = ui, server = server) Save the file and run the below line to the console to get the app running. library(shiny) runApp(&quot;shiny_apps/assignment/app.R&quot;) Here is how the app will look like ________________________________________________________________________________ "],["building-r-packages.html", "Chapter 8 Building R Packages 8.1 Introduction 8.2 Prerequisites 8.3 Building a simple R package 8.4 Making A New R Project 8.5 Adding Documentation 8.6 Uploading and Installing from Github 8.7 Hands-on Exercises", " Chapter 8 Building R Packages 8.1 Introduction In the second chapter we introduced about functions and we later talked about package functions, here we will now focus on how to build those packages. Packages are bundles of code and data to perform created by R users or community to perform a set of goven tasks. In this course you have encountered several packages like dplyr, plyr and ggplot2 and might have installed one or many of them. Packages provide a ready-to-use functions and data sets that produce results faster without the need to write everything from scratch. Here we will discuss how you can create you own package in R. This will give you a deeper appreciation of the packages you rely on daily basis and how they are built. CRAN(The Comprehensive R Archive Network) and GitHub provide a repository where one can host and install packages to their local environments. In this guide, we will walk through how to create a package and host the packages either on GitHub. 8.2 Prerequisites Before we jump in, there are a few packages you will want to have ready to help us along the way. We will install devtools and roxygen2. Install the packages # Install devtools install.packages(&quot;devtools&quot;) # Install roxygen install.packages(&quot;roxygen2&quot;) It might be necessary to restart R studio after installing the above packages 8.3 Building a simple R package Here is where the fun begin! We will create a simple R package to serve you with the basics of building packages. The package will involve two functions; A function to convert temperature from degrees Fahrenheit(\\(^oF\\)) to degrees Celsius(\\(^oC\\)). A function to convert temperature from degrees Celsius(\\(^oC\\)) to degrees Fahrenheit(\\(^oF\\)). Step 1 Create a folder that will hold the whole R package, in our case we will name the folder F2C_R_package like the one below. Step 2 Currently this folder is empty, lets create another folder inside this F2C_R_package folder and name it R. Here is where all the R scripts are stored. It can hold any amount of R files and each R file can hold any amount of R functions. You could example give each function each own file or insert more than function in a file. For large projects it is recommended to group similar functions in the same R file. Step 3 In our new R package, we will write both the two functions discussed above in the same file called, temp_conversion.R that has the code below; # Convert from Fahrenheit to Celsius F_to_C &lt;- function(F_temp){ C_temp &lt;- (F_temp -32) * 5/9 return (C_temp) } # Convert from Celsius to Fahrenheit C_to_F &lt;- function(C_temp){ F_temp &lt;- (C_temp * 9/5) + 32 return(F_temp) } That is the whole file for now, it has less than 15 lines of code. Step 4 Next, create a file called DESCRIPTION in the F2C_R_package directory. This will be a plain text file with no extension and it will hold some of the metadata on the R package. In our case, it will hold the following lines of code specifying the package name, type, title and the version number. Package: F2CTempConverter Type: Package Title: Temperature Conversion Package for Demonstration Version: 0.0.1.0 This is now a working R package and can be loaded by; library(devtools); load_all(&quot;F2C_R_package&quot;) # Load the directory path to where the package is ## ℹ Loading F2CTempConverter # Lets convert the temperature from Fahrenheit to Celsius F_to_C(79) ## [1] 26.11111 # Converting from Celsius to Fahrenheit C_to_F(20) ## [1] 68 After this its good to add documentation to help users know what the function does. 8.4 Making A New R Project What we just did was an overview of how a package is created. Lets now explore how you can create a complete package with documentation. To do this go to File &gt; New Project ... and the dialog box below should pop up. We will choose the Existing Directory option then browse to the F2C_R_package and click Create Project to create project from an existing directory. Now you should be able to see the project inside the package directory. 8.5 Adding Documentation Documentation helps others use the package that we have built, furthermore we can refer to the documentation to refer what we did after a long time. Documentation shows up in the Help tab of R Studio when running the function help. Just run the following in R studio to understand what I meant. help(lm) ? does the same thing as help ?lm You see there a tab that pops up with a documentation on Linear Models The roxygen2 package is used to make these helpful markdown files. We will then add some explanations to the code file, temp_conversion.R. Update the code to look the one below. #&#39; Fahrenheit conversion #&#39; #&#39; Convert degrees Fahrenheit temperatures to degrees Celsius #&#39; @param F_temp The temperature in degrees Fahrenheit #&#39; @return The temperature in degrees Celsius #&#39; @examples #&#39; temp1 &lt;- F_to_C(50); #&#39; temp2 &lt;- F_to_C( c(50, 63, 23) ); #&#39; @export F_to_C &lt;- function(F_temp){ C_temp &lt;- (F_temp - 32) * 5/9; return(C_temp); } #&#39; Celsius conversion #&#39; #&#39; Convert degrees Celsius temperatures to degrees Fahrenheit #&#39; @param C_temp The temperature in degrees Celsius #&#39; @return The temperature in degrees Fahrenheit #&#39; @examples #&#39; temp1 &lt;- C_to_F(22); #&#39; temp2 &lt;- C_to_F( c(-2, 12, 23) ); #&#39; @export C_to_F &lt;- function(C_temp){ F_temp &lt;- (C_temp * 9/5) + 32; return(F_temp); } The size of the code has increased but we now have helpful reminders to use each function. Lets go through each line type and explain; #' Fahrenheit conversion and #' Celsius conversion are the function titles. @param F_temp and @param C_temp are the function parameters or arguments . ' @return ... is used to indicate the return value of the function. #' @examples ... is shows an example use case of the function. The format explained above is called the Roxygen format and there are more tags recognized by the roxygen2 package. For more information visit Karl Broman’s page. This format makes it easy to create files in markdown for documentation. Now that we have the documentation ready, lets open the project as guided earlier and run the following in the console. library(roxygen2); # Read in the roxygen2 R package roxygenise(); # Builds the help files and here is how our package will look like. “man” and a plain text NAMESPACE have been added. The man directory holds the written help files while the NAMESPACE works with R to integrate them into the package correctly. The NAMESPACE file should not be edited by hand. Here are the contents of the namespace file. # Generated by roxygen2: do not edit by hand export(C_to_F) export(F_to_C) Inside the man directory there are two markdown files for the C_to_F and F_to_C functions. Feel free to load the package library(devtools); load_all(&quot;F2C_R_package&quot;) as earlier and ask for help with F_to_C. ?F_to_C The below will be presented in the ‘Help’ tab of R Studio. 8.6 Uploading and Installing from Github Now that we have package read and working we can upload to github for sharing and version control. Follow the steps below to get your package hosted and ready for sharing. Step 1: Create a github repo Visit github and sign in(or sign up if you don’t have an account) Click the New button to create a new repository. Name the repository ideally the same as the package name. In our case we will name our repository F2C_R_package. Add a description and make the repository public so that anyone can access it. Don’t initialize a README file since you will pushing your files from R Studio. Finally click Create repository. Step 2: Set up Git in R Studio Open your package project in R Studio. Go to Tools &gt; Project Options &gt; Git/SVN and select Git to enable version control. To commit you package to GitHub, first make sure you have git installed locally then link your R Studio project to your GitHub repository using the following commands in the terminal. # Intialize git in the project git init # Add all the files git add . # Commit files with a message git commit -m &quot;First upload of F2C_R_package&quot; # Add your github repository as the remote git remote add origin https://github.com/your-username/F2C_R_package.git # Push your commit to github git push -u origin main Replace your-username with your actual Github username. This will push the package files to GitHub. Step 3: Install your Package from Github on Any Machine Now, that your package is ready on Github, you can use the devtools package in R: Open an R session. Install devtools if you haven’t already(we installed it earlier) install.packages(&quot;devtools&quot;) Use devtools::install_github to install the packge from Github. devtools::install_github(&quot;your-username/F2C_R_package&quot;) Remember to replace \"your-username\" with you actual github details. R will download and install the package from GitHub. You can now load it via library(F2C_R_package) Additional Tips remember to update your repo whenever you make changes to the package. Add a README file for documentation on how the file works and how to install it. You package is now on GitHub and ready for installation and use! 8.7 Hands-on Exercises Create a package called weight converter that converts mass from Kilograms(kg) to Pounds(lb) and vice versa. Tag it as version 0.0.1 in the DESCRIPTION file. Add necessary documentations Upload it to Github Add README file to show how to install and use the file Install on your local machine from GitHub using devtools library. Finally, test your newly installed package and run some code. Solution Instruction to; Test the functionalities (Function to convert Kilos to pounds and back) Check documentation Check version tagged in the DESCRIPTION file. Check if package pushed to github(optional) Complete Solution in K2P_R_package directory Here is how the package is created; Install the packages required to build the app. # Install devtools install.packages(&quot;devtools&quot;) # Install roxygen install.packages(&quot;roxygen2&quot;) Create a folder and name it K2P_R_package. In the K2P_R_package directory, create a directory called R. In the R directory, created an R script file called mass_converter.R. Add the lines below to the mass_converter.R to create a function K_to_P that converts mass from Kilo to Pounds. K_to_P &lt;- function(K_mass){ P_mass &lt;- (K_mass * 2.20462) return (P_mass) } Add the lines below to the mass_converter.R to create a function P_to_K that converts mass from Pounds to Kilos. P_to_K &lt;- function(P_mass){ K_mass &lt;- (P_mass/2.20462) return (K_mass) } Under the K2P_R_package directory, add a file named DESCRIPTION and add the following lines; Package: K2PMassConverter Type: Package Title: Convert mass from kilo to pounds and vice versa Version: 0.0.1 RoxygenNote: 7.3.2 This is now a working file and can be add loaded to the R workspace by; library(devtools); load_all(&quot;K2P_R_package&quot;) # Load the directory path to where the package is Test the functionalities of the package by trying to convert mass from Kilos to Pounds and back. K_to_P(53) # Convert from Kilos to Pounds P_to_K(156) # COnvert from Pounds to Kilos The package can be uploaded to github or CRAN for collaboration. In this case, upload to github but first open it as a project. You will click File&gt;New Project&gt;Existing Directory then select the K2P_R_package folder. Add a documentation to the package by adding these lines above the K_to_P function. #&#39; Kilogram conversion #&#39; #&#39; Convert mass in Kilogram to Pounds #&#39; @param K_mass The mass in Kilograms #&#39; @return The mass in Pounds #&#39; @examples #&#39; mass1 &lt;- K_to_P(69); #&#39; mass2 &lt;- K_to_P( c(55, 69, 71) ); #&#39; @export Add these lines above the P_to_K function #&#39; Pounds conversion #&#39; #&#39; Convert mass in Pounds to Kilograms #&#39; @param P_mass The mass in Pounds #&#39; @return The mass in Kilograms #&#39; @examples #&#39; mass1 &lt;- P_to_K(150); #&#39; mass2 &lt;- P_to_K( c(200, 155, 342) ); #&#39; @export The whole mass_converter.R script will look like this #&#39; Kilogram conversion #&#39; #&#39; Convert mass in Kilogram to Pounds #&#39; @param K_mass The mass in Kilograms #&#39; @return The mass in Pounds #&#39; @examples #&#39; mass1 &lt;- K_to_P(69); #&#39; mass2 &lt;- K_to_P( c(55, 69, 71) ); #&#39; @export K_to_P &lt;- function(K_mass){ P_mass &lt;- (K_mass * 2.20462) return (P_mass) } #&#39; Pounds conversion #&#39; #&#39; Convert mass in Pounds to Kilograms #&#39; @param P_mass The mass in Pounds #&#39; @return The mass in Kilograms #&#39; @examples #&#39; mass1 &lt;- P_to_K(150); #&#39; mass2 &lt;- P_to_K( c(200, 155, 342) ); #&#39; @export P_to_K &lt;- function(P_mass){ K_mass &lt;- (P_mass/2.20462) return (K_mass) } To the console while the project is still open run the lines below to package the help file and the whole package library(roxygen2); # Read in the roxygen2 R package roxygenise(); # Builds the help files Visit github and create a new repo, finally commit your work. To find the documentation to specific functions on the K2P_R_package run ?K_to_P Try that to the P_to_K function. ________________________________________________________________________________ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
